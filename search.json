[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Probability and Statistics",
    "section": "",
    "text": "Overview\nWelcome to IPS!\nThis web site is used to provide some of the course materials, and should be used alongside the module’s VLE page. All of the written assignment submission points can be found on the VLE, along with the quizzes for completion as you work through the computer labs.\nYou only need to use this site to access the computer lab material. However, you will also be able to access copies of the written assignments here in html format, in case you find that more accessible than the pdf files which will be available on the VLE.\n\n\n\n\n\n\nNote\n\n\n\nYou can access the pdf version of any page of this site by clicking on the pdf icon in the left-hand menu. You can also choose to view the page in dark mode, if that’s more comfortable.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "labs/labs.html",
    "href": "labs/labs.html",
    "title": "Computer labs",
    "section": "",
    "text": "Assessment\nThe goal of these labs is to introduce you to, and build up your proficiency with, R and RStudio. You’ll be using these throughout the course, both to learn the statistical concepts discussed in the lectures and also to analyze real data and come to informed conclusions. To straighten out which is which:\nThe R language is the standard statistical tool used by most statisticians at universities. One reason data scientists and statisticians like to use R is that all known statistical techniques are available in R. Whenever someone develops a new statistical technique, one of the first things they do is produce an R package so that the technique becomes available in R. The reason they do this for R rather than for one of the commercial alternatives is that R is open source and freely available to all, and of course that the previous methods on which the new method builds are already available in R.\nFeeling comfortable using R is not only important for this module and any further statistics modules you may take at the Department of Mathematics of the University of York, it can also be an important factor for your future career (see the article “R skills attract the highest salaries”. Even though R is specially designed for statistics, it is consistently in the list of the top ten most important programming languages compiled by the IEEE spectrum magazine.\nAs the labs progress, you are encouraged to explore beyond what the labs dictate; a willingness to experiment will make you a much better programmer.\nThe Intro lab does not count for credit, but you should attempt this in the first week of the semester to make sure that:",
    "crumbs": [
      "Computer labs"
    ]
  },
  {
    "objectID": "labs/labs.html#assessment",
    "href": "labs/labs.html#assessment",
    "title": "Computer labs",
    "section": "",
    "text": "Important\n\n\n\nThe five main labs (imaginatively named “Lab 1” to “Lab 5”) count for credit: your best 4 out of 5 will marks will count for 20% of the module mark.\nEach lab will have an accompanying quiz. As you work through each lab you will find places where you are asked to perform a calculation and then enter your mark in the appropriate quiz.\nYou can have two attempts at each quiz; the mark from your best attempt will become your final grade for that lab.\n\n\n\n\nyou can successfully access R\nyou know how to enter answers in the accompanying quiz.",
    "crumbs": [
      "Computer labs"
    ]
  },
  {
    "objectID": "labs/intro-lab.html",
    "href": "labs/intro-lab.html",
    "title": "Intro Lab: Meeting R and RStudio",
    "section": "",
    "text": "The data: Dr. Arbuthnot’s baptism records\nIn this introduction we begin with the fundamental building blocks of R and RStudio: the interface, reading in data, and basic commands.\nThe first step is to open RStudio.\nOnce you’ve opened RStudio, you should see a window similar to that depicted below.\nA good way to work through these labs is is to have this file open on one half of your screen and RStudio on the other half. On a PC you can usually move a window to the left or right half of the screen by holding down the Windows key and pressing the left or right arrow key.\nThe panel in the upper right of the RStudio window contains your Environment as well as a History of the commands that you’ve previously entered. The lower right panel has several tabs, including Plots where any plots that you generate will show up.\nThe panel on the left is where the action happens. It’s called the Console. Every time you launch RStudio, it will have text at the top of the console giving lots of information that you can mostly ignore, including the version of R that you’re running. Below that information is the prompt. As its name suggests, this prompt is really a request, a request for a command. Initially, interacting with R is all about typing commands and interpreting the output. These commands and their syntax have evolved over decades (literally) and now provide what many users feel is a fairly natural way to access data and organize, describe, and invoke statistical computations.\nTo get you started, enter the following command at the R prompt (i.e. right after &gt; on the console). You can either type it in manually or copy and paste it from this document.\nThis command instructs R to access the OpenIntro website and fetch some data: the Arbuthnot baptism counts for boys and girls. You should see that the environment area in the upper right hand corner of the RStudio window now lists a data set called arbuthnot that has 82 observations on 3 variables.\nAs you interact with R, you will create a series of objects. Sometimes you load them as we have done here, and sometimes you create them yourself as the by-product of a computation or some analysis you have performed.\nNote that because it is accessing data on the web, the above command will work in a computer lab, in the library, or at home; just as long as you have access to the internet.\nThe Arbuthnot data set was compiled by Dr. John Arbuthnot, an 18th century physician, writer, and mathematician. He was interested in the ratio of newborn boys to newborn girls, so he gathered the baptism records for children born in London for every year from 1629 to 1710. We can take a look at the data by typing its name into the console and hitting Enter.\narbuthnot\nWhat you should see are four columns of numbers, each row representing a different year: the first entry in each row is simply the row number (an index we can use to access the data from individual years if we want), the second is the year, and the third and fourth are the numbers of boys and girls baptised that year, respectively. Use the scroll bar on the right side of the console window to examine the complete data set.\nMoving back to the console, if we only want to see the first few lines of the data set, we can type\nhead(arbuthnot)\n#&gt;   year boys girls\n#&gt; 1 1629 5218  4683\n#&gt; 2 1630 4858  4457\n#&gt; 3 1631 4422  4102\n#&gt; 4 1632 4994  4590\n#&gt; 5 1633 5158  4839\n#&gt; 6 1634 5035  4820\nSometimes, as in this example, I’ll show you the output of the commands when I run them on my computer, so that you can compare with what you get when you run the commands yourself: any line starting with #&gt; corresponds to code output.\nNote that the row numbers in the first column are not part of Arbuthnot’s data. R adds them as part of its printout to help you make visual comparisons. You can think of them as the index that you see on the left side of a spreadsheet. In fact, the comparison to a spreadsheet will generally be helpful. R has stored Arbuthnot’s data in a kind of spreadsheet or table called a data frame.\nYou can see the dimensions of this data frame by typing:\ndim(arbuthnot)\n#&gt; [1] 82  3\nThis indicates that there are 82 rows and 3 columns (we’ll get to what the [1] means in a bit), just as it says next to the object in your Environment tab. You can see the names of these columns (or variables) by typing:\nnames(arbuthnot) \n#&gt; [1] \"year\"  \"boys\"  \"girls\"\nYou should see that the data frame contains the columns year, boys, and girls. By this point, you might have noticed that many of the commands in R look a lot like functions; that is, invoking R commands means supplying a function with some number of arguments. The dim() and names() commands, for example, each took a single argument, the name of a data frame.",
    "crumbs": [
      "Computer labs",
      "Intro Lab: Meeting R and RStudio"
    ]
  },
  {
    "objectID": "labs/intro-lab.html#the-data-dr.-arbuthnots-baptism-records",
    "href": "labs/intro-lab.html#the-data-dr.-arbuthnots-baptism-records",
    "title": "Intro Lab: Meeting R and RStudio",
    "section": "",
    "text": "Tip\n\n\n\nA nice feature of RStudio is that it comes with a built-in data viewer. Click on the name arbuthnot in the upper right window that lists the objects in your environment. This will bring up an alternative display of the Arbuthnot counts in the upper left panel of the RStudio window.\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIn the html version of this document, the word head() in the code block above is underlined (as is the command source() further up the page). Clicking on an R command which is underlined will take you to its online documentation, where you can read more about how to use it.",
    "crumbs": [
      "Computer labs",
      "Intro Lab: Meeting R and RStudio"
    ]
  },
  {
    "objectID": "labs/intro-lab.html#some-exploration",
    "href": "labs/intro-lab.html#some-exploration",
    "title": "Intro Lab: Meeting R and RStudio",
    "section": "Some exploration",
    "text": "Some exploration\nLet’s start to examine the data a little more closely. We can access the data in a single column of a data frame separately using a command like\n\narbuthnot$boys\n\nThis command will only show the number of boys baptised each year.\n\n\n\n\n\n\nYour turn\n\n\n\nWhat command would you use to extract just the counts of girls baptised each year? Try it!Now answer quiz question 1.\n\n\nNotice that the way R has printed these data is different. When we looked at the complete data frame, we saw 82 rows, one on each line of the display. These data are no longer structured in a table with other variables, so they are displayed one right after another.\nObjects that print out in this way are called vectors; they represent a set of numbers. R has added numbers in [brackets] along the left side of the printout to indicate locations within the vector. For example, 5218 follows [1], indicating that 5218 is the first entry in the vector. And if [43] starts a line, then that would mean the first number on that line would represent the 43rd entry in the vector.\nR has some powerful functions for making graphics. We can create a simple plot of the number of girls baptised per year with the command\n\nplot(x = arbuthnot$year, y = arbuthnot$girls) \n\n\n\n\n\n\n\nBy default, R creates a scatterplot with each (x,y) pair indicated by an open circle. The plot itself should appear under the Plots tab of the lower right panel of RStudio.\nNotice that the command above again looks like a function, this time with two arguments separated by a comma. The first argument in the plot function specifies the variable for the x-axis and the second for the y-axis. If we wanted to connect the data points with lines, we could add a third argument, the letter l for line.\n\nplot(x = arbuthnot$year, y = arbuthnot$girls, type = \"l\")\n\n\n\n\n\n\n\nYou might wonder how you are supposed to know that it was possible to add that third argument. Thankfully, R documents all of its functions extensively: you’ve already seen that clicking on any of the underlined commands in this page takes you to the relevant entry in the documentation. Another way to read what a function does, and learn the arguments that are available to you, is to just type in a question mark followed by the name of the function that you’re interested in. Try the following.\n\n?plot\n\nCan you figure out how to produce a plot that shows both the points and the lines connecting them?\nNotice that the help file replaces the plot in the lower right panel. You can toggle between plots and help files using the tabs at the top of that panel.\n\n\n\n\n\n\nYour turn\n\n\n\nIs there an apparent trend in the number of girls baptised over the years?Answer quiz question 2.\nCan you also guess, just by looking at the graph, when the English civil war started?\n\n\nNow, suppose we want to plot the total number of baptisms. To compute this, we could use the fact that R is really just a big calculator. We can type in mathematical expressions like\n\n5218 + 4683\n\nto see the total number of baptisms in 1629. We could repeat this once for each year, but there is a faster way. If we add the vector for baptisms for boys and girls, R will compute all sums simultaneously.\n\narbuthnot$boys + arbuthnot$girls\n\nWhat you will see are 82 numbers (in that packed display, because we aren’t looking at a data frame here), each one representing the sum we’re after. Take a look at a few of them and verify that they are right.\nWe can now make a plot of the total number of baptisms per year with the command\n\nplot(arbuthnot$year, arbuthnot$boys + arbuthnot$girls, type = \"l\")\n\nThis time, note that we left out the names of the first two arguments. We can do this because the help file shows that the default for plot is for the first argument to be the x-variable and the second argument to be the y-variable.\nNext we calculate the proportion of the baptised children that are boys. We can do this for the year 1629 with the command\n\n5218 / (5218 + 4683)\n\nbut this may also be computed for all years simultaneously:\n\narbuthnot$boys / (arbuthnot$boys + arbuthnot$girls)\n\nNote that with R, as with your calculator, you need to be conscious of the order of operations. Here, we want to divide the number of boys by the total number of newborns, so we have to use parentheses. Without them, R will first do the division, then the addition, giving you something that is not a proportion.\n\n\n\n\n\n\nYour turn\n\n\n\nNow, make a plot of the proportion of boys over time. The command for making the plot will be similar to the plot command you used earlier, just with a different expression for the y argument.Now answer quiz question 3.\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf you use the up and down arrow keys, you can scroll through your previous commands, your so-called command history. You can also access it by clicking on the History tab in the upper right panel. This will save you a lot of typing in the future.\n\n\nIn addition to simple mathematical operators like subtraction and division, you can ask R to make comparisons like greater than, &gt;, less than, &lt;, and equality, == (note that it has to be a double equal sign, not a single equal sign). For example, we can ask if boys outnumber girls in each year with the expression\n\narbuthnot$boys &gt; arbuthnot$girls\n\nThis command returns 82 values of either TRUE if that year had more boys than girls, or FALSE if that year did not (the answer may surprise you). This output shows a different kind of data than we have considered so far. In the arbuthnot data frame our values are numerical (the year, the number of boys and girls). Here, we’ve asked R to create logical data, data where the values are either TRUE or FALSE. In general, data analysis will involve many different kinds of data types, and one reason for using R is that it is able to represent and compute with many of them.\nYou can count the number of entries for which the condition is TRUE by just summing the entries in the vector\n\nsum(arbuthnot$boys &gt; arbuthnot$girls)\n\nThe reason this works is that R automatically converts TRUE to 1 and FALSE to 0 when asked to do a numerical calculation with these values.\n\n\n\n\n\n\nYour turn\n\n\n\nAbove you have seen how to calculate the proportion of newborns that are boys. You have also learned how to count the number of entries in the data that satisfy a particular condition.Now combine those two to answer quiz question 4.",
    "crumbs": [
      "Computer labs",
      "Intro Lab: Meeting R and RStudio"
    ]
  },
  {
    "objectID": "labs/intro-lab.html#a-newer-data-set",
    "href": "labs/intro-lab.html#a-newer-data-set",
    "title": "Intro Lab: Meeting R and RStudio",
    "section": "A newer data set",
    "text": "A newer data set\nIn the previous few pages, you recreated some of the displays and preliminary analysis of Arbuthnot’s baptism data. To practise your new skills, you will now repeat these steps, but for present day birth records in the United States. Load up the present day data with the following command.\n\nsource(\"http://www.openintro.org/stat/data/present.R\") \n\nThe data are stored in a data frame called present.\n\n\n\n\n\n\nYour turn\n\n\n\n\nWhat years are included in this data set? What are the dimensions of the data frame and what are the variable or column names?\nHow do these counts compare to Arbuthnot’s? Are they on a similar scale?\nDoes Arbuthnot’s observation about boys being born in greater proportion than girls hold up in the U.S.?\nMake a plot that displays the boy-to-girl ratio for every year in the data set. What do you see?\nWhat was the largest total number of births in a single year in the U.S. during the period covered by the dataset? You can refer to the help files or the R reference card to find helpful commands.\n\nNow answer questions 5 and 6 in the quiz.\n\n\nThese data come from a report by the Centers for Disease Control. Check it out if you would like to read more about an analysis of sex ratios at birth in the United States.\nTo exit RStudio you can click the cross in the upper right corner of the whole window. You will be prompted to save your workspace. If you click save, RStudio will save the history of your commands and all the objects in your workspace so that the next time you launch RStudio, you will see arbuthnot and you will have access to the commands you typed in your previous session.",
    "crumbs": [
      "Computer labs",
      "Intro Lab: Meeting R and RStudio"
    ]
  },
  {
    "objectID": "labs/lab1.html",
    "href": "labs/lab1.html",
    "title": "Lab 1: Script files and simulation",
    "section": "",
    "text": "Working with an R script file\nThis lab has three goals:\nEspecially the use of variables can be confusing, because, as the name “variable” indicates, the value of a variable can change over time.\nI assume that you have already worked carefully through the previous lab so that you know how to open RStudio and execute some R commands. Again I would recommend that while working through this lab you keep this pdf file open on one half of your screen and RStudio on the other half. So now go ahead and open RStudio.\nIn the previous lab you worked directly in the console. For this lab you will be working in an R script file. An R script file is simply a text file that contains the commands that you want R to execute. The advantage of typing the R commands into the script file and executing them from there rather than typing them straight into the console is that in the script file you can lay out your calculations in an understandable way and you can revisit your calculations easily later to build on them or to share them with others.\nThe first step is to create a new R script file. To do that you click on the left-most icon on the toolbar at the top of the RStudio window, the one that looks like a piece of paper with a plus sign . That opens a drop-down menu. The top entry is R script and is the one you want to select. This will open an editor panel above your console with a new empty text file. That is where you will type in the R commands for this lab.\nFor a first example of using a script file, let’s use R to simulate the experiment of drawing a ball at random from a bag containing 4 red, 6 green and 3 blue balls. (We’ll look further into the idea of simulation later on in this lab; for now, just follow the instructions to get familiar with using a script file.)\nLet’s combine these commands to create our bag; we will store this in a variable, that we choose to call bag, so that we can use it in what follows. We can also sample from the bag, and save the outcome in the variable x. Copy the following code into your script file:\n#  Code to simulate the experiment of drawing balls at random \n#  from a bag containing 4 red, 6 green and 3 blue balls.\n\n#  First create the variable 'bag', which lists all ball colours:\nbag &lt;- c(rep(\"red\", 4), rep(\"green\", 6), rep(\"blue\", 3))\n\n#  Draw a ball at random from bag, and assign this to variable 'x':\nx &lt;- sample(bag, size = 1)\nNow let’s look at the code that you’ve just pasted into your script file. There are a few important things to notice here.\nSo far you have only put the code into your R script file – R has not yet evaluated the code. For that you should click somewhere in the first line of your code and then click the Run icon on the tool bar or, alternatively, hold down the Ctrl key and hit Enter. Either method will send that line of code to the R console and run it. (Notice that R skips the first few lines of comments, and only evaluates the line beginning bag.) It will also move the cursor to the next line, so that you can then execute the second line by again clicking Run or pressing Ctrl-Enter. Each time you send one of the commands to the console you should see a new variable appear in the Environment panel.\nNow let’s suppose that we actually wanted to draw not one, but 100 balls from the bag (replacing the ball that we’ve withdrawn each time). We can just go back to our script and edit the final line (and its comment!) as follows:\n#  Draw 100 balls at random from bag, and assign this to variable 'x':\nx &lt;- sample(bag, size = 100, replace = TRUE)\nSuppose that we want to calculate the frequencies with which we see each colour. Here’s one possibility for calculating the proportion of red balls:\n#  Calculate proportion of red balls in x:\nred_prop &lt;- sum(x == \"red\") / 100\nA more direct route is to use R’s built-in function table(). This calculates counts of each distinct element in x; we can then divide by the number of draws to obtain the proportions.\n#  Calculate counts of each colour in x:\nx_counts &lt;- table(x)\n#  Now turn these into proportions:\nx_props &lt;- x_counts / length(x)\nx_props\nNote that I’ve used length(x) to calculate the number of elements in x: here we know that’s 100, but writing it this way means that if I want to go back and change the number of samples, I don’t have to remember to also change that number when calculating the proportions.",
    "crumbs": [
      "Computer labs",
      "Lab 1: Script files and simulation"
    ]
  },
  {
    "objectID": "labs/lab1.html#working-with-an-r-script-file",
    "href": "labs/lab1.html#working-with-an-r-script-file",
    "title": "Lab 1: Script files and simulation",
    "section": "",
    "text": "We can use the rep() function to create a vector with repeated entries. For example rep(\"red\", 4).\nWe can use the c() function to concatenate several vectors.\nWe can use the sample() function to choose a random element from a vector.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nSave the R script file frequently by clicking on the floppy disk icon  on the toolbar. The first time you save the document you will be prompted to choose a file name and location:\n\nuse an informative file name: don’t just name it after yourself – you’ll be creating lots of script files during this module, and in your future studies! A good name for this script might be IPS_lab1.R, or similar. (Note that R script files always have file extension .R.)\nif you are on a campus PC and save the document to your H: drive then you will be able to access it from any other campus PC or even from your home PC. For details see this IT Services page.\n\n\n\n\n\nNotice the &lt;- syntax for assigning a value to a variable. We will make a lot of use of that in the future. Many other programming languages use the syntax =.\nEverything after a hash symbol # is ignored by R, so the hash symbol is used to start comments that explain your R code. Commenting your code is a VERY good idea. When you come back to look at your code again later you will be very glad that you left comments documenting what you were thinking when you originally wrote the code.\nYou probably also noticed the way I used extra spaces to align the code across the lines. Those spaces have no function, other than making the code more readable.\n\n\n\n\n\n\n\n\nTip\n\n\n\nInstead of sending one line of code to the console at a time, you can also highlight multiple lines in the editor and hit Run just once.\n\n\n\n\n\n\n\n\n\n\n\n\nYour turn\n\n\n\nAdd lines to your script file to calculate the proportions of blue and green balls in your vector x.\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can download my R script file for all of the above here, and compare it to yours.\n\n\n\n\n\n\n\n\nYour turn\n\n\n\nNow add six additional yellow balls to the bag you used so far. Then record the outcome of 100,000 repetitions of the experiment of drawing a ball from that bag. Calculate the proportion of those 100,000 draws that gave a yellow ball.Answer quiz question 1.",
    "crumbs": [
      "Computer labs",
      "Lab 1: Script files and simulation"
    ]
  },
  {
    "objectID": "labs/lab1.html#simulation",
    "href": "labs/lab1.html#simulation",
    "title": "Lab 1: Script files and simulation",
    "section": "Simulation",
    "text": "Simulation\nWe all have the intuitive idea that if we make many independent repetitions of a probability experiment, then the long run frequencies of events will be similar to their probabilities. This is indeed true, and we will investigate this formally in the lectures later when we prove the Law of large numbers. This means that one way to perform some of the more complicated probability calculations would be to just re-run the experiment many times to determine the frequencies of events. This is often known as the Monte Carlo method.\nMaking many independent repetitions of a probability experiment is tedious. It takes a long time to throw a die 100,000 times. So we will instead ask the computer to simulate the experiments, as we did above with the simple example of drawing balls from a bag.\nIn this document I am not only showing R commands that I want you to use, but I also show the output of those commands, preceded by #&gt;, as well as the figures produced by plots. I nevertheless strongly recommend that you also evaluate the commands yourself and reproduce those outputs.\nSimulating random samples\nThe first question we need to address is how to generate random numbers; this is a difficult problem, but one that has been extensively studied.\nOne way to generate random numbers would be to have an actual physical device in the computer that performs repeated measurements of some physical quantity whose distribution is well known. For example it is known that the arrival times of radioactive particles measured in a Geiger counter is exponentially distributed. (We’ll meet the exponential distribution later in this course.)\nAn alternative and more convenient way to generate random numbers is to use a computer algorithm to produce a sequence of numbers that, while not truly random, is practically indistinguishable from a sequence of random numbers. They are not truly random numbers because if the same algorithm is run again with the same initial condition, it will produce the same sequence again. This initial condition is called the seed for the random number generator.\nMost computer languages have good random number generators built in. This is of course particularly true for R. In fact, it has a whole range of different algorithms for generating random numbers. By default it uses the Mersenne-Twister algorithm.\nThere are functions in R to create samples from all of the common discrete and continuous probability distributions that we’ll meet later on in this module, and it is also possible to specify your own distribution and sample from that. We will see examples of that later in this lab.\nFirst we want to simulate a die. So we want to draw from the sample space \\(\\{1,2,3,4,5,6\\}\\) with equal probability. A quick way to generate the set of integers \\(\\{m,m+1,m+2,\\dots,n-1,n\\}\\) in R is to use the command m:n. So with \\(m=1\\) and \\(n=6\\) we can obtain our sample space by typing\n\n1:6\n#&gt; [1] 1 2 3 4 5 6\n\n\n\n\n\n\n\nNote\n\n\n\nWe could also have used the very useful function seq() to do this job for us. Take a look at its documentation to see some examples of how it can be used.\n\n\nNow that we have our sample space, we can use the sample function, as we saw above. The following produces a sample of size 30:\n\nsample(1:6, 30, replace = TRUE)\n#&gt;  [1] 2 1 1 4 4 4 1 2 6 4 2 4 6 3 5 4 5 6 3 6 5 6 6 2 5 2 5 3 5 3\n\nGo ahead and put this command into a new R script file and send the command to the console repeatedly. A different random sample is produced each time.\n\n\n\n\n\n\nTip\n\n\n\nA convenient way to send a chunk of code to the console repeatedly is to use the Re-run previous code section button, right next to the Run button.\n\n\nNow try\n\nset.seed(42)\nsample(1:6, 30, replace = TRUE)\n#&gt;  [1] 1 5 1 1 2 4 2 2 1 4 1 5 6 4 2 2 3 1 1 3 4 5 5 5 4 2 4 3 2 1\n\nand notice that each time you reset the seed to 42 you get the same sequence of pseudo random numbers. Try changing the seed to a different number and see that that produces a different sample. If you want to repeat the same sample, you have to set the seed to the same value right before creating the sample, because each time you generate a random number the seed changes.\nWhenever a lab introduces a new function, like sample() above, I recommend that you take a look at the help page for that function. To find the help for the function, you can\n\ntype the function name into the console or the script file editor and then hit the F1 key;\nor click on the function, if it appears in R code in one of these labs and is underlined.\n\nDoing the first of these will open the help page in the Help tab in the frame on the lower right of the RStudio window; the second will take you to the online documentation page. The help page first gives a brief description of the function, then sample usage, then explains the arguments that the function can take, then provides more detailed explanations and finally, at the bottom, provides examples. I usually do not read all the details, but I have a look at the list of arguments and at some of the examples.\nI strongly recommend that, in order to get a feel for the new function you just learned about, you start playing with it a bit by using it with different arguments. So for example you might try\n\nsample(c(\"H\",\"T\"), 10, replace = TRUE)\n#&gt;  [1] \"T\" \"T\" \"T\" \"H\" \"H\" \"T\" \"T\" \"T\" \"T\" \"T\"\n\nto create a sample of 10 coin flips. Or\n\nsample(c(\"red\",\"red\", \"red\", \"blue\",\"blue\"), 2, replace = FALSE)\n#&gt; [1] \"red\" \"red\"\n\nto draw two balls at random (without replacement) out of a bag containing three red and two blue balls. Experimentation is the best way to get friendly with the computer.\n\n\n\n\n\n\nYour turn\n\n\n\nAnswer quiz question 2.\n\n\nThe following code sets the seed, sets the sample size to 30, creates a random sample, assigns it to the variable x, tables the frequency of each value, and then makes a barplot of the result.\n\nset.seed(1)\nn &lt;- 30\nx &lt;- sample(1:6, n, replace = TRUE)\nbarplot(table(x))\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAs always, you should be adding each line of code to your script file, so that you can easily re-run it later if necessary. Add your own comments to remind you what each chunk of code does!\n\n\nEstimating probabilities from a random sample\nNext let’s estimate probabilities of various events by counting how frequently they occur in the sample.\nLet’s start by calculating the probability of the event that the die shows a number less or equal to 3. So our sample space is \\(\\Omega = \\{1,2,3,4,5,6\\}\\), and our event of interest is \\(E=\\{1,2,3\\}\\): we want to estimate \\(\\mathbb{P}\\left(E\\right)\\). We will use a trick that you met already in the first lab when you counted how many years had more newborn boys than girls. We create a vector of 0s and 1s in which a 1 in a particular place indicates that the event has taken place in that particular repetition of the experiment:\n\ny &lt;- as.numeric(x &lt;= 3); y\n#&gt;  [1] 1 0 1 1 0 1 0 1 1 1 1 0 0 1 0 0 1 1 0 0 1 1 0 0 0 1 1 0 1 0\n\nThen we calculate the proportion of repetitions for which the event has taken place by summing over all entries in the vector (hence counting the 1s) and then dividing by the size of the sample:\n\nsum(y)/n\n#&gt; [1] 0.5333333\n\nThis gives the best approximation to the probability \\(\\mathbb{P}\\left(E\\right)\\) that we can obtain from this sample. It is close to but not exactly equal to the theoretical value of 0.5.\n\n\n\n\n\n\nYour turn\n\n\n\nAnswer quiz question 3.\n\n\nWe can make a plot that shows how the approximation to the probability behaves as the sample size grows:\n\nyn &lt;- cumsum(y)/(1:n)\nplot(yn, type = \"b\", ylim = c(0,1), \n     xlab = \"Sample size\", ylab = \"Proportion less than or equal to 3\")\nabline(h = 1/2, lty = \"dotted\")\n\n\n\n\n\n\n\nThis shows that while the values in the random sample keep fluctuating, the estimate of the probability settles down towards its true value as the sample size increases.\nThe first line of the code above produces a vector of values whose ith entry is the proportion of 1s in the first i values in the vector y. It then assigns this vector of proportions to the variable yn. You do not have to understand the command in detail, unless you want to.\nThe second line produces the plot of the values, where we have asked R to show both the points and the straight lines joining them, and to limit the range of the y-axis to the interval (0,1). We’ve also added more informative labels to the axes.\nFinally, the last line abline(h = 1/2, lty = \"dotted\") draws a dotted horizontal line at the height 0.5 to indicate the theoretical answer to \\(\\mathbb{P}\\left(E\\right)\\).\nNow play around by producing similar plots for larger sample size.\nWe can similarly calculate the probability that the die shows a six with\n\ny &lt;- as.numeric(x == 6); y\n#&gt;  [1] 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0\nsum(y)/n\n#&gt; [1] 0.1666667\n\nThe correct value of course is \\(1/6\\approx 0.167\\). We see that the sample is really too small to give a reliable estimate of the probability of obtaining a six. So we redo this with a larger sample of size 1,000:\n\nn &lt;- 1000\nset.seed(1)\nx &lt;- sample(1:6, n, replace=TRUE)\ny &lt;- as.numeric(x == 6)\nsum(y)/n\n#&gt; [1] 0.164\n\nThe following code performs the calculation of the estimated probability for all values from 1 to 6 and plots them in a bar plot.\n\nbarplot(table(x)/n, ylab = \"Estimated probability\")\n\n\n\n\n\n\n\nBetter, but still not a very good approximation to the theoretical answer. This illustrates that one needs very large sample sizes to get reliable results. Repeat this with larger samples to see how the estimates improve.\n\n\n\n\n\n\nYour turn\n\n\n\nSet the seed to 12. Produce a sample of size 1,000,000 for the experiment of rolling a fair 6-sided die. What proportion of rolls give the outcome 6?Answer quiz question 4.\n\n\nWe can also use our sample to approximate the probability of more complicated events. For example, suppose that we wish to consider the event that the outcome of a fair die roll is a 2 or a 3. That is, we want to estimate \\(\\mathbb{P}\\left(\\{2,3\\}\\right)\\). We can do this by counting the numbers of 2s and 3s in our sample\n\nsum(x == 2 | x ==3)\n#&gt; [1] 316\n\nNote that we’ve used the symbol | to mean or. So sum(x == 2 | x ==3) counts how many entries in x are equal to 2 or equal to 3. Similarly, we can use the symbol != to mean not equal, and the symbol & to mean and. So\n\nsum(x &gt; 1 & x &lt; 4)\n#&gt; [1] 316\n\nis another way of counting the number of 2s and 3s, while\n\nsum(x != 5)\n#&gt; [1] 824\n\ncounts the number of outcomes in x that are not equal to 5.\n\n\n\n\n\n\nYour turn\n\n\n\nAnswer quiz question 5.\n\n\nAnother probability problem\nSimulation provides a lazy way of “solving” probability problems. Take for example the following problem.\n\nA shop receives a batch of 1,000 cheap lamps. The chance that any given lamp is defective is 0.1%. What is the probability that there are more than two defective lamps in the batch?\n\nWe can easily simulate a batch of 1,000 cheap lamps. Let us represent a defective lamp by 1 and a working lamp by 0.\n\nset.seed(0)\nlamps &lt;- sample(c(0, 1), 1000, replace = TRUE, prob = c(0.999, 0.001))\nlamps\n#&gt;    [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n#&gt;   [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n#&gt;   [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n#&gt;  [112] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n#&gt;  [149] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n#&gt;  [186] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n#&gt;  [223] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n#&gt;  [260] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n#&gt;  [297] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n#&gt;  [334] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n#&gt;  [371] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n#&gt;  [408] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n#&gt;  [445] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n#&gt;  [482] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n#&gt;  [519] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n#&gt;  [556] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n#&gt;  [593] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n#&gt;  [630] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n#&gt;  [667] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n#&gt;  [704] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n#&gt;  [741] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n#&gt;  [778] 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n#&gt;  [815] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n#&gt;  [852] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n#&gt;  [889] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n#&gt;  [926] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n#&gt;  [963] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n#&gt; [1000] 0\n\nWe can then count how many defective lamps are in that batch.\n\nsum(lamps)\n#&gt; [1] 2\n\nThere were 2 defective lamps in that sample. Now, without resetting the seed, we take another sample to represent another random batch of lamps and again count the defective lamps.\n\nlamps &lt;- sample(c(0, 1), 1000, replace = TRUE, prob = c(0.999, 0.001))\nsum(lamps)\n#&gt; [1] 0\n\n0 in this batch. Let’s try another\n\nsum(sample(c(0, 1), 1000, replace = TRUE, prob = c(0.999, 0.001)))\n#&gt; [1] 1\n\nThe replicate() function allows us to repeat this a chosen number of times and collect the results into a vector.\n\nset.seed(0)\nreplicate(20, sum(sample(c(0,1), 1000, replace = TRUE, prob = c(0.999, 0.001))))\n#&gt;  [1] 2 0 1 1 0 0 0 1 0 0 0 0 1 2 0 1 2 1 0 2\n\nSo no batch with more than 2 defective lamps in the first 20 batches. Now we will simulate 100,000 batches and then count the number of batches with more than 2 defective lamps.\n\nset.seed(0)\ncount_defective &lt;- replicate(100000, sum(sample(c(0,1), 1000, replace = TRUE, prob = c(0.999, 0.001))))\nsum(count_defective &gt; 2)\n#&gt; [1] 8022\n\nWe can use this to estimate the probability of getting more than two defective lamps in a batch by dividing this by the total number of batches\n\nsum(count_defective &gt; 2) / 100000\n#&gt; [1] 0.08022\n\n\n\n\n\n\n\nWhat answer should we expect here?\n\n\n\nIf \\(X\\) is the number of defective lamps in a batch of size 1,000, then you may already know that \\(X\\) will follow a binomial distribution with parameters 1,000 and 0.001: \\(X\\sim\\mbox{\\textup{Bin}}(1000,0.001)\\). (Don’t worry if this doesn’t mean anything to you: we’ll be learning about this properly later in the course!)\nWe can write\n\\[\\mathbb{P}\\left(X&gt;2\\right) = 1-\\mathbb{P}\\left(X=0\\right)-\\mathbb{P}\\left(X=1\\right)-\\mathbb{P}\\left(X=2\\right)\\,. \\] So to calculate this we need to be able to evaluate the probability mass function of this binomially distributed random variable. Of course R has a function for this, called dbinom(). So we can calculate the probability that a batch has more than 2 defective lamps as\n\n1 - dbinom(0, 1000, 0.001) - dbinom(1, 1000, 0.001) - dbinom(2, 1000, 0.001)\n#&gt; [1] 0.08020934\n\nIn fact, R also has a function pbinom() for calculating the distribution function. So we could also have calculated \\(\\mathbb{P}\\left(X&gt;2\\right) = 1- \\mathbb{P}\\left(X\\le 2\\right)\\) with\n\n1 - pbinom(2, 1000, 0.001)\n#&gt; [1] 0.08020934\n\nOf course in this example it was faster to solve the problem by using the binomial distribution instead of by simulation, but there are many real-world probability problems that can not be solved analytically and for which simulation is the only viable approach.\n\n\n\n\n\n\n\n\nYour turn\n\n\n\nAnswer quiz question 6.",
    "crumbs": [
      "Computer labs",
      "Lab 1: Script files and simulation"
    ]
  },
  {
    "objectID": "labs/lab2.html",
    "href": "labs/lab2.html",
    "title": "Lab 2: Introduction to data",
    "section": "",
    "text": "The Behavioral Risk Factor Surveillance System\nSome define Statistics as the field that focuses on turning information into knowledge. This worksheet is designed to give you more practice with summarising and visualising the raw information - the data. In this lab, you will gain insight into public health by generating simple graphical and numerical summaries of a data set collected by the Centers for Disease Control and Prevention (CDC). As this is a large data set, along the way you’ll also learn the indispensable skills of data processing and subsetting.\nThe Behavioral Risk Factor Surveillance System (BRFSS) is an annual telephone survey of 350,000 people in the United States. As its name implies, the BRFSS is designed to identify risk factors in the adult population and report emerging health trends. For example, respondents are asked about their diet and weekly physical activity, their HIV/AIDS status, possible tobacco use, and even their level of healthcare coverage. The BRFSS web site contains a complete description of the survey, including the research questions that motivate the study and many interesting results derived from the data.\nWe will focus on a random sample of 20,000 people from the BRFSS survey conducted in 2000. While there are over 200 variables in this data set, we will work with a small subset.\nWe begin by loading the data set of 20,000 observations into the R workspace. Loading the data set may take a few seconds, so be patient. Use the following command to load the data:\nsource(\"http://www.openintro.org/stat/data/cdc.R\")\nOnce loaded, the data set cdc shows up in your Environment panel. It is in a format that R calls a data frame. It is a table with each row representing a case and each column representing a variable. We can have a look at the first few entries (rows) of our data with the command\nhead(cdc)\nand similarly we can look at the last few by typing\ntail(cdc)\nYou could also look at all of the data frame at once by typing its name into the console, but that might be unwise here: we know cdc has 20,000 rows, so viewing the entire data set would mean flooding your screen. It’s better to take small peeks at the data with head, tail or the subsetting techniques that you’ll learn in a moment.",
    "crumbs": [
      "Computer labs",
      "Lab 2: Introduction to data"
    ]
  },
  {
    "objectID": "labs/lab2.html#the-behavioral-risk-factor-surveillance-system",
    "href": "labs/lab2.html#the-behavioral-risk-factor-surveillance-system",
    "title": "Lab 2: Introduction to data",
    "section": "",
    "text": "Types of variables\nYou already know from the Intro Lab that to view the names of the variables in our data set you can type the command\n\nnames(cdc)\n\nThis returns the names genhlth, exerany, hlthplan, smoke100, height, weight, wtdesire, age, and gender. Each one of these variables corresponds to a question that was asked in the survey. For example, for genhlth, respondents were asked to evaluate their general health, responding either excellent, very good, good, fair or poor. The exerany variable indicates whether the respondent exercised in the past month (1) or did not (0). Likewise, hlthplan indicates whether the respondent had some form of health cover plan (1) or did not (0). The smoke100 variable indicates whether the respondent had smoked at least 100 cigarettes in their lifetime (1) or had not (0). The other variables record the respondent’s height in inches, weight in pounds as well as their desired weight, wtdesire, age in years, and gender.\nVariables come in different types. It is important to distinguish between different types of variables since methods for viewing and summarising data are dependent on variable type. A variable is either quantitative or qualitative.\nA variable that is quantitative (numeric) may be either discrete or continuous. A discrete variable is a numerical variable that can assume a finite number or at most a countably infinite number of values, for example, the number of students in a class. A continuous variable is a numerical variable that can assume an uncountable number of values associated with subsets of the real number line, for example, the height of a tree.\nWhen a variable is qualitative, it is essentially defining groups or categories. Qualitative variables are therefore also often referred to as categorical variables. When the categories have no ordering the variable is called nominal. For example, a variable “music preference” could have values such as “classical,” “jazz,” “rock,” or “other.” When the categories have a distinct ordering, the variable is called ordinal. Such a variable might be educational level with values GCSEs, A-levels, Bachelors degree, Masters degree, PhD.\nThe distinction between the different types is not always as clear cut as one would like. Consider for example the variable height that represents the respondents’ height in inches. Even though this is always rounded to integer values in the data set, it is still a continuous variable, because non-integer values would make sense, even though they may not be used in the data set.\nNote that even categorical variables can take numerical values, because the categories could be labelled by numbers. We see this for example in the variable exerany that takes the values 0 and 1, with 1 representing that the respondent has exercised in the last month and 0 that they have not. This is a categorical variable. It is less clear whether it is ordinal or nominal, but luckily for a variable that takes on only two possible values the distinction is of no consequence. Only once there are at least three values will the statistical techniques differ between ordinal and nominal variables.\n\n\n\n\n\n\nYour turn\n\n\n\nLook at the variables in this data set. For each variable, identify its data type. How many of the variables are quantitative? How many are categorical?Answer quiz question 1.\n\n\nSummaries and tables\nThe BRFSS questionnaire is a massive trove of information. A good first step in any analysis is to distil all of that information into a few summary statistics and graphics.\nAs a simple example, the function summary() returns a numerical summary: minimum, first quartile, median, mean, second quartile, and maximum. For weight this is\n\nsummary(cdc$weight)\n\nWe will look more closely at the meaning of these summary statistics later.\nWhile it makes sense to describe a quantitative variable like weight in terms of these statistics, what about categorical data? We would instead consider the sample frequency or relative frequency distribution. The function table() does this for you by counting the number of times each kind of response was given. For example, to see the number of people who have smoked 100 cigarettes in their lifetime, type\n\ntable(cdc$smoke100)\n\nor instead look at the relative frequency distribution by typing\n\ntable(cdc$smoke100)/20000\n\nNotice how R automatically divides all entries in the table by 20,000 in the command above. This is similar to something we have already observed; when we multiplied or divided a vector by a number, R applied that action across all entries in the vector. As we see above, this also works for tables. Next, we make a bar plot of the entries in the table by putting the table inside the barplot() command.\n\nbarplot(table(cdc$smoke100))\n\nNotice what we’ve done here! We’ve computed the table of cdc$smoke100 and then immediately applied the graphical function, barplot. This is an important idea: R commands can be nested. You could also break this into two steps by typing the following:\n\nsmoke &lt;- table(cdc$smoke100)\nbarplot(smoke)\n\nHere, we’ve made a new object, a table, called smoke (the contents of which we can see by typing smoke into the console) and then used it in as the input for barplot.\n\n\n\n\n\n\nYour turn\n\n\n\nCreate numerical summaries for height and age. Compute the relative frequency distribution for gender and exerany. Answer quiz question 2.\n\n\nThe table command can be used to tabulate any number of variables that you provide. For example, to examine which participants have smoked across each gender, we could use the following.\n\ntable(cdc$gender, cdc$smoke100)\n\nHere, we see column labels of 0 and 1. Recall that 1 indicates a respondent has smoked at least 100 cigarettes. The rows refer to gender. To create a mosaic plot of this table, we would enter the following command.\n\nmosaicplot(table(cdc$gender, cdc$smoke100))\n\nWe could have accomplished this in two steps by saving the table in one line and applying mosaicplot in the next (see the table/barplot example above).\nWe can also use a barplot to show how respondents’ general health differs by gender:\n\nbarplot(table(cdc$genhlth, cdc$gender),\n        beside = F,\n        legend.text = T,\n        xlab = \"Gender\",\n        ylab = \"Frequency\",\n        main = \"General health by gender\")\n\n\n\n\n\n\n\nYour turn\n\n\n\nTry changing beside = F to beside = T and see what changes. Which do you find more informative?\n\n\n\n\n\n\n\n\nTip\n\n\n\nNote that you can flip between plots that you’ve created by clicking the forward and backward arrows in the Viewer window of RStudio, just above the plots.",
    "crumbs": [
      "Computer labs",
      "Lab 2: Introduction to data"
    ]
  },
  {
    "objectID": "labs/lab2.html#interlude-how-r-thinks-about-data",
    "href": "labs/lab2.html#interlude-how-r-thinks-about-data",
    "title": "Lab 2: Introduction to data",
    "section": "Interlude: how R thinks about data",
    "text": "Interlude: how R thinks about data\nWe mentioned that R stores data in data frames, which you might think of as a type of spreadsheet. Each row is a different observation (a different respondent) and each column is a different variable (the first is genhlth, the second exerany, and so on). We can see the size of the data frame next to the object name in the workspace or we can type\n\ndim(cdc)\n\nwhich will return the number of rows and columns. Now, if we want to access a subset of the full data frame, we can use row-and-column notation. For example, to see the sixth variable of the 567th respondent, use the format\n\ncdc[567, 6] \n\nwhich means we want the element of our data set that is in the 567th row (meaning the 567th person or observation) and the 6th column (in this case, weight). We know that weight is the 6th variable because it is the 6th entry in the list of variable names:\n\nnames(cdc)[6]\n\nTo see the weights for the first 10 respondents we can type\n\ncdc[1:10, 6]\n\nIn this expression, we have asked just for rows in the range 1 through 10. We’ve already seen that R uses the : notation to create a range of values, so 1:10 expands to 1, 2, 3, 4, 5, 6, 7, 8, 9, 10. You can see this by entering\n\n1:10\n\nFinally, if we want all of the data for the first 10 respondents, type\n\ncdc[1:10, ] \n\nBy leaving out an index or a range (we didn’t type anything between the comma and the closing square bracket), we get all the columns. When starting out in R, this can be a bit counterintuitive. As a rule, we omit the column number to see all columns in a data frame. Similarly, if we leave out an index or range for the rows, we would access all the observations, not just the 567th, or rows 1 through 10. Try the following to see the weights for all 20,000 respondents fly by on your screen\n\ncdc[ , 6]\n\nR recognises that it is not very useful to put so many numbers on the screen, so stops after 1,000 entries.\nRecall that column 6 represents respondents’ weight, so the command above reported all of the weights in the data set. We have already seen an alternative method to access the weight data by referring to the name. We can use any of the variable names to select items in our data set, for example\n\ncdc$weight\n\nThe dollar-sign $ tells R to look in data frame cdc for the column called weight. Since that’s a single vector, we can subset it with just a single index inside square brackets. We see the weight for the 567th respondent by typing\n\ncdc$weight[567]\n\nSimilarly, for just the first 10 respondents\n\ncdc$weight[1:10] \n\nThe command above returns the same result as the cdc[1:10, 6] command.\n\n\n\n\n\n\nTip\n\n\n\nBoth row-and-column notation and dollar-sign notation are widely used: which one you choose to use depends on your personal preference, but in the above example the dollar-sign version does have the advantage of making clear the variable name.\n\n\n\n\n\n\n\n\nYour turn\n\n\n\nAnswer quiz question 3.\n\n\nA little more on subsetting\nIt’s often useful to extract all observations (cases) in a data set that have specific characteristics. We accomplish this through conditioning commands. First, consider expressions like\n\ncdc$gender == \"m\" \n\nor\n\ncdc$age &gt; 30\n\nAs we saw in Lab 1, these commands produce vectors of TRUE and FALSE values. There is one value for each respondent, where TRUE indicates that the person was male (via the first command) or older than 30 (second command).\nSuppose we want to extract just the data for the men in the sample, or just for those over 30. We can use the R function subset() to do that for us. For example, the command\n\nmdata &lt;- subset(cdc, cdc$gender == \"m\") \n\nwill create a new data set called mdata that contains only the men from the cdc data set. In addition to finding it in your workspace alongside its dimensions, you can take a peek at the first several rows as usual\n\nhead(mdata)\n\nThis new data set contains all the same variables but just under half the rows. It is also possible to tell R to keep only specific variables, which is a topic we’ll discuss in a future lab. For now, the important thing is that we can carve up the data based on values of one or more variables.\nAs we saw in Lab 1, we can use several of these conditions together with & and |. The & is read and so that\n\nm_and_over30 &lt;- subset(cdc, cdc$gender == \"m\" & cdc$age &gt; 30)\n\nwill give you the data for men over the age of 30. The | character is read or so that\n\nm_or_over30 &lt;- subset(cdc, cdc$gender == \"m\" | cdc$age &gt; 30) \n\nwill take people who are men or over the age of 30 (why that’s an interesting group is hard to say, but right now the mechanics of this are the important thing). In principle, you may use as many “and” and “or” clauses as you like when forming a subset.\n\n\n\n\n\n\nYour turn\n\n\n\nCreate a new object called under23_and_smoke that contains all observations of respondents under the age of 23 that have smoked at least 100 cigarettes in their lifetime. Use the summary command to see the summary statistics for the weight variable in this smaller data set.Answer quiz question 4.",
    "crumbs": [
      "Computer labs",
      "Lab 2: Introduction to data"
    ]
  },
  {
    "objectID": "labs/lab2.html#creating-new-variables-from-old",
    "href": "labs/lab2.html#creating-new-variables-from-old",
    "title": "Lab 2: Introduction to data",
    "section": "Creating new variables from old",
    "text": "Creating new variables from old\nSometimes we wish to use variables in our dataset to create new measurements of interest. We’ve seen that each variable in our dataset is stored as a column in the cdc data frame: each column can be easily accessed using either row-and-column or dollar-sign notation, and then manipulated as we would a vector. This means that it is simple to perform simple algebraic operations on variables to create new ones.\nFor example, suppose that we wish to create a new variable, weight_centred, which measures the difference between a person’s weight and the mean weight of the entire sample. We can do this by typing\n\nweight_centred &lt;- cdc$weight - mean(cdc$weight)\n\nWe call such a variable centred because it has been shifted so as to have zero mean:\n\nsummary(weight_centred)\n\n(Note that if you type mean(weight_centred) then R returns the value \\(-5.2492 \\times 10^{-15}\\) instead of zero: this is just an artefact caused by rounding.)\n\n\n\n\n\n\nYour turn\n\n\n\nCreate a new variable called male_height_centred that measures the difference between each male respondent’s height and the mean height of all male respondents. Answer quiz question 5.\n\n\nNow let’s consider a new variable: the difference between desired weight (wtdesire) and current weight (weight). Create this new variable by subtracting the two columns in the data frame and assigning them to a new object called wdiff.\n\nwdiff &lt;- cdc$weight - cdc$wtdesire\n\nWe could then count how many people currently weigh more than their desired weight:\n\nsum(wdiff &gt; 0)\n\n\n\n\n\n\n\nYour turn\n\n\n\nWhat proportion of female respondents have a current weight which is exactly the same as their desired weight?Answer quiz question 6.\n\n\nFinally, let’s consider another new variable that doesn’t show up directly in this data set: Body Mass Index (BMI). BMI is a weight to height ratio and can be calculated as\n\\[\\text{BMI} = \\frac{weight~(lb)}{height~(in)^2} \\times 703\\] where 703 is the approximate conversion factor to change units from metric (metres and kilograms) to imperial (inches and pounds).\n\n\n\n\n\n\nYour turn\n\n\n\nCreate a variable bmi which gives the BMI of each respondent in the dataset. (Hint: to square each element of a vector x in R you can type x^2.) Check that the mean BMI value of the cdc respondents is 26.30693.Answer quiz question 7.\n\n\nSuppose that we now choose one of the respondents in the cdc dataset at random: define \\(A\\) to be the event\n\\[A = \\{\\text{the BMI of our randomly chosen respondent is greater than 34}\\}\\,.\\]\nWhat is \\(\\mathbb{P}\\left(A\\right)\\)? Since each person in the dataset is equally likely to be chosen, we can calculate this probability by counting how many respondents have a BMI greater than 34, and dividing by the total number of respondents:\n\nsum(bmi &gt; 34)/20000\n#&gt; [1] 0.0756\n\nYour final exercise for this lab involves calculating a conditional probability. Recall that we already saw that the mean BMI value is 26.30693. Define the event \\(B\\) by\n\\[B=\\{\\text{the BMI of our randomly chosen respondent is greater than the mean value}\\}\\,.\\]\n\n\n\n\n\n\nYour turn\n\n\n\nAnswer quiz question 8.",
    "crumbs": [
      "Computer labs",
      "Lab 2: Introduction to data"
    ]
  },
  {
    "objectID": "labs/lab3.html",
    "href": "labs/lab3.html",
    "title": "Lab 3: Data and distributions",
    "section": "",
    "text": "Numerical summaries of data\nIn the first part of this worksheet you will look in more detail at various numerical and graphical summaries of data. This reinforces, and slightly expands on, what you have already met in Lab 2 and is closely related to the material from chapter 16 in the textbook.\nIn the second part you will get a first glimpse at how statistics makes a connection between probability theory and data: you will model the height variable in a dataset as a normally distributed random variable.",
    "crumbs": [
      "Computer labs",
      "Lab 3: Data and distributions"
    ]
  },
  {
    "objectID": "labs/lab3.html#numerical-summaries-of-data",
    "href": "labs/lab3.html#numerical-summaries-of-data",
    "title": "Lab 3: Data and distributions",
    "section": "",
    "text": "Datasets\nData, according to The American Heritage Dictionary, is “Information, especially information organised for analysis or used as the basis for a decision”. Data comes in all sizes and shapes. Often it is disorganised and difficult to work with. The first step in data analysis is then to clean and organise the data. We will assume that the data has already been organised into what we call a dataset. A dataset consists of a number \\(n\\) of observations of the values of one or more variables.\nHere is an example of a small multivariate dataset, collected from a previous year of IPS students:\n\n\nHeight(cm)\nAge(years)\nEye colour\n# of textbooks\nLikes Stats\n\n\n\n175\n19\nGreen\n1\nVery much\n\n\n160\n23\nBrown\n2\nNot at all\n\n\n180\n21\nBlue\n1\nA bit\n\n\n159\n18\nBrown\n7\nA bit\n\n\n\nThis dataset contains \\(n=4\\) observations of five different variables. The variables are the height, the age and the eye colour of students in the class, the number of probability textbooks they had looked at for this course, and the answer to the question: “How much do you like Statistics?”.\nBelow you will see how this data is stored in R as a data frame. A data frame is just like the above table, but with a bit of extra information about the types of the different variables.\nTypes of variables\nRecall from Lab 2 that a variable is either quantitative or qualitative. It is important to distinguish between different types of variables since methods for viewing and summarising data are dependent on variable type.\nThe quantitative variables in our example dataset are “Height”, “Age” and “Number of textbooks”. Of these “Height” and “Age” are continuous variables; even if the height might be given rounded to the nearest centimetre it would still be thought of as a continuous variable because non-integer values would make sense. The “Number of textbooks” variable is discrete.\nLet us tell R about the values of these quantitative variables:\n\nheight &lt;- c(175, 160, 180, 159) \nage &lt;- c(19, 23, 21, 18) \nnum_books &lt;- c(1, 2, 1, 7) \n\nThe R function c() binds together its arguments into a vector. You already know how to work with such vectors. For example, to get the height of the 3rd student you evaluate\n\nheight[3] \n#&gt; [1] 180\n\nThe qualitative variables in our example dataset are “Eye colour” and “Likes Stats”. Of these, “Eye colour” is nominal and “Likes Stats” is ordinal. R refers to categorical variables as factors. We can tell R that a variable is qualitative with the factor() function as follows:\n\neye_col &lt;- factor(c('Green', 'Brown', 'Blue', 'Brown'))\n\nIf you look for the eye_col variable in the Environment tab of your RStudio window you will see that it is listed as: Factor w/ 3 levels \"Blue\",\"Brown\",...: 3 2 1 2.\nFor the “Likes Stats” variable we also need to tell R how to order the categories.\n\nlikes_stats &lt;- factor(c('Very much', 'Not at all', 'A bit', 'A bit'), \n                      levels = c('Not at all', 'A bit', 'Very much'), \n                      ordered = TRUE)\n\nThis allows R to know whether the second student likes statistics more than the fourth for example:\n\nlikes_stats[2] &gt; likes_stats[4]\n#&gt; [1] FALSE\n\nWe can now bind all these variables together into a data frame:\n\nstudents &lt;- data.frame(height, age, eye_col, num_books, likes_stats)\n\nGiven a dataset, we want to make sense of it. We begin by summarizing the distribution of the variables in the dataset. The first questions one would ask are: are the values centred around a particular value, and then how much variation around that central value is there?\nCentral value of a variable\nThe most important way to define a “central” value for a collection of values for a quantitative variable is the mean, which is an average of the values. If we have \\(n\\) observations of a variable \\(X\\), denoted by \\(x_1,x_2, \\dots, x_n\\), then the mean is\n\\[ \\bar{x}_n=(x_1+\\dots+x_n)/n.\\]\nTake the variable “Age” from our example dataset. There we have\n\\[ \\bar{x}_4=(19+23+21+18)/4=81/4=20.25.\\]\n\nx &lt;- students$age\nmean(x)\n#&gt; [1] 20.25\n\nOne drawback of using the mean to define the centre of a dataset is that the mean can get very much affected by extreme values. For example, if we added a fifth datapoint to the above dataset, a mature student aged 58, then the mean would change to\n\\[\\bar{x}_5=(19+23+21+18+58)/5=139/5=27.8.\\]\n\nxl &lt;- c(x, 58) # this appends the number 58 to the end of the vector x\nmean(xl)\n#&gt; [1] 27.8\n\nThis value is outside the cluster of values around \\(20\\). An alternative to the mean that is less affected by such outliers is the median. To define this we first list the values in ascending order. We enclose the indices in this ordered set in parentheses to distinguish them from the indices of the values in the unordered dataset. So we have the values\n\\[ x_{(1)}\\leq x_{(2)}\\leq\\cdots\\leq x_{(n)}.\\]\nThe kth value \\(x_{(k)}\\) is often referred to as the kth order statistic. In our example\n\\[x_{(1)}=18, x_{(2)}=19, x_{(3)}=21, x_{(4)}=23, x_{(5)}=58.\\]\n\nsort(xl)\n#&gt; [1] 18 19 21 23 58\n\nThe median is defined as\n\\[\n\\text{Med}_n=\\begin{cases}x_{((n+1)/2)}&\\text{ if $n$ is odd}\\\\\\frac{1}{2}\\left(x_{(n/2)}+x_{(n/2+1)}\\right)&\\text{ if $n$ is even}\\end{cases}\n\\]\nSo for our small dataset of four age values we have\n\\[\n\\text{Med}_4=\\frac{1}{2}\\left(x_{(2)}+x_{(3)}\\right)=\\frac{1}{2}\\left(19+21\\right)=20.\n\\]\n\nmedian(x)\n#&gt; [1] 20\n\nFor the larger dataset of five values including the outlier, we have\n\\[\\text{Med}=x_{(3)}=21 \\]\nstill quite close to the centre of the cluster of values.\n\nmedian(xl)\n#&gt; [1] 21\n\n\n\n\n\n\n\nYour turn\n\n\n\nWhat is the median height of the first four students?Answer quiz question 1.\n\n\nAmount of variability in a variable\nThe most important measure of the variability in the data around the central value is the sample variance\n\\[\ns_n^2=\\frac{1}{n-1}\\sum_{i=1}^n(x_i-\\bar{x}_n)^2.\n\\]\nFor our sample data we get\n\\[\n  \\begin{split}\n  s_4^2 &= \\frac{1}{3}\\sum_{i=1}^4(x_i-\\bar{x}_4)^2 \\\\\n  &=\\frac{1}{3}\\left((19-20.25)^2+(23-20.25)^2+(21-20.25)^2+(18-20.25)^2\\right)\\\\\n  &=\\frac{1}{3}\\left(\\left(\\frac{5}{4}\\right)^2+\\left(\\frac{11}{4}\\right)^2+\\left(\\frac{3}{4}\\right)^2+\\left(\\frac{9}{4}\\right)^2\\right)\\\\\n  &=\\frac{236}{48}\\approx 4.917.\n  \\end{split}\n\\]\n\nvar(x)\n#&gt; [1] 4.916667\n\nThe age was measured in years. The variance is therefore measured in square years. It is often useful to have a measure of the variability that has the same units as the variable itself. Therefore one defines the sample standard deviation \\(s_n\\) to be the square root of the sample variance, \\(s_n=\\sqrt{s_n^2}\\).\n\nsd(x) \n#&gt; [1] 2.217356\n\nLike the mean, the variance is affected very much by outliers. In our example with the extra datapoint \\(x_5=58\\) we find\n\nvar(xl) \n#&gt; [1] 288.7\nsd(xl)\n#&gt; [1] 16.99117\n\nA measure that is less affected by outliers is the median of absolute deviations,\n\\[ \\text{MAD}_n=\\text{Med}\\left(|x_1-\\text{Med}_n|,\\dots,|x_n-\\text{Med}_n|\\right).\\]\nIn our example datasets\n\\[\\text{MAD}_4=\\text{Med}\\left(|19-20|,|23-20|,|21-20|,|18-20|\\right)=\\text{Med}(1,3,1,2)=1.5,\\]\nand\n\\[\n\\text{MAD}_5=\\text{Med}\\left(|19-21|,|23-21|,|21-21|,|18-21|,|50-21|\\right)=\\text{Med}(2,2,0,3,29)=2.\n\\]\n\nmad(x, constant=1)\n#&gt; [1] 1.5\nmad(xl, constant=1)\n#&gt; [1] 2\n\nWe see that the outlier does not affect the median of absolute deviation nearly as much as it affects the standard deviation.\n\n\n\n\n\n\nYour turn\n\n\n\nWhat is the variance in the height of the first four students?Answer quiz question 2.\n\n\nEmpirical quantiles, quartiles, and IQR\nWe now introduce the quantiles which give us more detailed information about the distribution of values. The \\(p\\)-th quantile is a value so that a proportion \\(p\\) of the values in the dataset is below or equal to this value.\n\n\n\n\n\n\nNote\n\n\n\nBecause there are gaps between the values in the dataset, there is not a unique such value. R provides nine different types of quantiles. We present here the one that R uses as its default.\n\n\nFor \\(p\\in[0,1]\\) we define the \\(p\\)-th quantile as\n\\[ q_n(p)=x_{(k)}+\\alpha\\left(x_{(k+1)}-x_{(k)}\\right) \\]\nwhere\n\\[ k=\\lfloor h\\rfloor,~~~\\alpha=h-\\lfloor h\\rfloor,~~~\\text{ with }h=(n-1)p+1.\\]\nRecall that \\(\\lfloor{x}\\rfloor\\) (the floor of \\(x\\)) denotes the largest integer smaller or equal to \\(x\\).\nThe \\(p\\)-th quantile is also referred to as the \\(100p\\)-th percentile. Three percentiles are given special names:\n\nlower quartile = 25th percentile \\(=q_n(0.25)\\)\n\nmedian = 50th percentile \\(=q_n(0.5)\\)\n\nupper quartile = 75th percentile \\(=q_n(0.75)\\).\n\nWe calculate the upper and lower quartiles in our example dataset containing the age of \\(n=4\\) students. For the lower quartile we have \\(p=1/4\\) and we calculate\n\\[ h=(n-1)p+1=\\frac{3}{4}+1, ~~~k=\\lfloor h\\rfloor = 1,~~~\\alpha=h-\\lfloor h\\rfloor = \\frac34.\\] Then\n\\[ q_4(0.25)=x_{(1)}+\\alpha\\left(x_{(2)}-x_{(1)}\\right)=18+0.75(19-18)=18.75.\\]\nFor the upper quartile, \\(p=3/4\\), we find similarly\n\\[ h=(n-1)p+1=3\\frac{3}{4}+1, ~~~k=\\lfloor h\\rfloor] = 3,~~~\\alpha=h-\\lfloor h\\rfloor = \\frac14, \\] and \\[ q_4(0.75)=x_{(3)}+\\alpha\\left(x_{(4)}-x_{(3)}\\right)=21+0.25(23-21)=21.5.\\]\nTo let R do the calculation for us we use\n\nquantile(x)\n#&gt;    0%   25%   50%   75%  100% \n#&gt; 18.00 18.75 20.00 21.50 23.00\n\nThe 0% quantile is the minimum value and the 100% quantile the maximum value. These are also known as the range of the data.\n\nrange(x)\n#&gt; [1] 18 23\n\nBy default the quantile() command gives us the quartiles. To get a particular quantile use\n\nquantile(x, 0.35)\n#&gt;  35% \n#&gt; 19.1\n\n\n\n\n\n\n\nNote\n\n\n\nOur textbook1 uses a different convention for the quantiles that has \\(h=(n+1)p\\), which is type 6 in R.\n\nquantile(x, type=6)\n#&gt;    0%   25%   50%   75%  100% \n#&gt; 18.00 18.25 20.00 22.50 23.00\n\nUse the command ?quantile in R to get more information. When there are many values in the dataset then the difference between the alternative conventions will be negligible.\n\n\nThe interquartile range, abbreviated as IQR, is the difference between the upper and the lower quartile:\n\\[ IQR=q_n(0.75)-q_n(0.25).\\]\nIn our example we find \\(IQR=2.75\\).\n\nIQR(x)\n#&gt; [1] 2.75",
    "crumbs": [
      "Computer labs",
      "Lab 3: Data and distributions"
    ]
  },
  {
    "objectID": "labs/lab3.html#the-data",
    "href": "labs/lab3.html#the-data",
    "title": "Lab 3: Data and distributions",
    "section": "The data",
    "text": "The data\nThis week we’ll be working with measurements of body dimensions. This dataset contains measurements from 247 men and 260 women, most of whom were considered healthy young adults. The data is saved in an RData file. We download it from the internet and then load it into R.\n\ndownload.file(\"http://www.openintro.org/stat/data/bdims.RData\", \n              destfile = \"bdims.RData\")\nload(\"bdims.RData\")\n\nLet’s take a quick peek at the first few rows of the data.\n\nhead(bdims)\n\nYou’ll see that for every observation we have 25 measurements, many of which are either diameters or girths. A key to the variable names can be found here, but we’ll be focusing on just three columns to get started: weight in kg (wgt), height in cm (hgt), and sex (1 indicates male, 0 indicates female).\nSince males and females tend to have different body dimensions, it will be useful to create two additional datasets: one with only men and another with only women.\n\nmdims &lt;- subset(bdims, bdims$sex == 1) \nfdims &lt;- subset(bdims, bdims$sex == 0) \n\nLet us take a quick look at some summary statistics of the women’s height.\n\nsummary(fdims$hgt) \n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;   147.2   160.0   164.5   164.9   169.5   182.9\n\nNote that this summary gives rounded results only. If you want more exact results you should use the functions we introduced above, for example\n\nmean(fdims$hgt)\n#&gt; [1] 164.8723\n\n\n\n\n\n\n\nYour turn\n\n\n\nAnswer quiz question 3.\n\n\nA so-called stem-and-leaf plot is one way of getting a quick view of the data.\n\nstem(fdims$hgt)\n#&gt; \n#&gt;   The decimal point is at the |\n#&gt; \n#&gt;   146 | 2\n#&gt;   148 | 59\n#&gt;   150 | 11\n#&gt;   152 | 0044444\n#&gt;   154 | 4599008\n#&gt;   156 | 0002250055555555\n#&gt;   158 | 028800124455588\n#&gt;   160 | 000000000000000000002277790022233333334\n#&gt;   162 | 00112556666666666666688900222255888\n#&gt;   164 | 0013455501111111111111157\n#&gt;   166 | 00244488015666666666666666668\n#&gt;   168 | 223599999999004555\n#&gt;   170 | 0000002222222222355948\n#&gt;   172 | 15777777790224\n#&gt;   174 | 0000000022233333\n#&gt;   176 | 2255558\n#&gt;   178 | 089\n#&gt;   180 | 3\n#&gt;   182 | 9\n\nHere the integer part of each data point is use as the stem and listed vertically. The last digit (the leaf) is printed behind the vertical bar, one for each observation with the same stem.\nThe box-and-whisker plot, or boxplot for short, is an aptly named graphical representation of the summary statistics we have just introduced. It consists of a box that extends in the vertical direction from the lower to the upper quartile, with a horizontal line through the box at the median.\nWhiskers may extend from the box:\n\nthe upper whisker extends to the highest value in the dataset no more than 1.5 IQR above the upper quartile. If there is no value in the dataset in this range then there will be no upper whisker.\nsimilarly the lower whisker extends to the lowest value in the dataset no more than 1.5 IQR below the lower quartile.\n\nFinally, any value in the dataset that falls outside the box and whiskers is drawn as a dot: these values are referred to as outliers.\n\nboxplot(fdims$hgt)\n\n\n\n\n\n\n\nThe purpose of a boxplot is to provide a thumbnail sketch of a variable for the purpose of comparing across several categories. So we can, for example, compare the heights of men and women with\n\nboxplot(bdims$hgt ~ bdims$sex)\n\n\n\n\n\n\n\nThe notation here is new. The ~ character can be read “versus” or “as a function of”. So we’re asking R to give us box plots of heights where the groups are defined by sex.",
    "crumbs": [
      "Computer labs",
      "Lab 3: Data and distributions"
    ]
  },
  {
    "objectID": "labs/lab3.html#data-and-probability-distributions",
    "href": "labs/lab3.html#data-and-probability-distributions",
    "title": "Lab 3: Data and distributions",
    "section": "Data and probability distributions",
    "text": "Data and probability distributions\nIt is natural to try to make a connection between variables in a dataset and random variables, and between observations of the variable in a dataset and a random sample from the random variable. So we will think of the observations in the dataset as being produced by a probability experiment. Or, said differently, we model the real-world variable as a random variable. The task of Statistics is to determine what the distribution of that random variable should be to best match the distribution of observed values in the dataset.\nThe normal distribution\nIn this lab we’ll investigate the probability distribution that is most central to statistics: the normal distribution. If we are confident that a variable in our dataset is well described by a normally distributed random variable, that opens the door to many powerful statistical methods. Here we’ll use the graphical tools of R to assess the normality of our data.\nWe’ll be working with women’s heights. We will try to model this as a normal random variable. To see how accurate that description is, we can plot a normal distribution curve on top of a histogram of the observed values to see how closely the data follow a normal distribution. This normal curve should have the same mean and standard deviation as the data. Let’s calculate these statistics so that we can easily use them later.\n\nfhgtmean &lt;- mean(fdims$hgt)\nfhgtsd   &lt;- sd(fdims$hgt)\n\n\n\n\n\n\n\nYour turn\n\n\n\nWhat is the standard deviation (in cm) in the observations of the women’s heights in the dataset?Answer quiz question 4.\n\n\nNext we make a density histogram to use as the backdrop and use the lines function to overlay a normal probability curve. The difference between a frequency histogram and a density histogram is that while in a frequency histogram the heights of the bars add up to the total number of observations, in a density histogram the areas of the bars add up to 1. Frequency and density histograms both display the same exact shape; they only differ in their y-axis. Using a density histogram allows us to properly overlay a density function curve over the histogram since it too is normalised to have an area of 1 under the curve. To produce a density histogram we use the hist() command, and include the parameter probability = TRUE:\n\nhist(fdims$hgt, probability = TRUE)\nx &lt;- 140:190\ny &lt;- dnorm(x, mean = fhgtmean, sd = fhgtsd)\nlines(x, y, col = \"blue\")\n\n\n\n\n\n\n\nIn the second and third lines of the above code we created the x- and y-coordinates for the normal curve. We chose the x range as 140 to 190 in order to span the entire range of fheight. To create y, we used dnorm to calculate the density of each of those x-values in a distribution that is normal with mean fhgtmean and standard deviation fhgtsd. The final command draws a curve on the existing plot (the density histogram) by connecting each of the points specified by x and y. The argument col simply sets the colour for the line to be drawn.If we left it out, the line would be drawn in black.\nThe top of the curve is cut off because the limits of the x- and y-axes are set to best fit the histogram. To adjust the y-axis you can add the ylim argument to the histogram function. We also put a better label on the x-axis and a better title.\n\nhist(fdims$hgt, probability = TRUE,  ylim = c(0, 0.06), \n     xlab=\"Women's height (in cm)\", \n     main=\"Histogram of women's height\")\nlines(x, y, col = \"blue\")\n\n\n\n\n\n\n\nBased on the this plot, it appears that the data are pretty well approximated by a normal distribution.\nSkewness\nNext let us make a similar plot for the female age variable.\n\nhist(fdims$age, probability = TRUE, \n     xlab=\"Women's age (in years)\", \n     main=\"Histogram of women's age\")\nx &lt;- 10:70\ny &lt;- dnorm(x, mean = mean(fdims$age), sd = sd(fdims$age))\nlines(x, y, col = \"blue\")\n\n\n\n\n\n\n\nThe age is clearly not normally distributed. The distribution of age is skewed towards the left, towards younger age.\nSuch an asymmetry in a distribution is measured by the coefficient of skewness. A distribution like the one above that has a heavier or longer tail on the right has a positive skewness. This is the first time that we meet a common phenomenon in R: much of R’s functionality is provided by additional packages. If you search for skewness in the R help system, you will see that there are two packages installed on the campus PCs that define a skewness function, the e1071 package and the timeDate packages. We will use the former.\n\ne1071::skewness(fdims$age)\n#&gt; [1] 1.185329\n\n\n\n\n\n\n\nTip\n\n\n\nIf you are working on your own computer, the package e1071 may not be installed yet. In that case you have to issue the following command before the above code will work:\n\ninstall.packages(\"e1071\")\n\n\n\nNote how the function name is preceded by the name of the package and two colons. An alternative way to use functions from a package is to load the package library:\n\nlibrary(e1071)\n\nThen we can use the function without a prefix:\n\nskewness(fdims$age)\n#&gt; [1] 1.185329\n\nData that has a distribution with a large skewness can not be well described by the normal distribution because the normal distribution is symmetric and hence has no skewness.\n\n\n\n\n\n\nYour turn\n\n\n\nAnswer quiz question 5.\n\n\nUsing Q-Q plots\nEyeballing the shape of the histogram is one way to determine if the data appear to be nearly normally distributed, but it can be frustrating to decide just how close the histogram is to the curve. An alternative approach involves constructing a so-called Q-Q plot (Q-Q stands for “Quantile-Quantile”). In a Q-Q plot the quantiles of one distribution are plotted against the quantiles of another: if the two distributions agree, then this should produce a straight line.\nIn our case we want to plot the quantiles of the women’s height distribution against those of the normal distribution. This is known as a normal Q-Q plot.\n\nqqnorm(fdims$hgt)\n\n\n\n\n\n\n\nThe quantiles of the standard normal distribution are plotted on the horizontal axis and the observed quantiles on the vertical axis. To see better how close this is to the straight line that would arise if the distribution was perfectly normal, R has a function to plot this straight line:\n\nqqline(fdims$hgt)\n\nThis plot for female heights shows points that tend to follow the line but with some errant points towards the tails.\nWe’re left with the same problem that we encountered with the histogram above: how close is close enough?\nA useful way to address this question is to rephrase it as: what would the probability plots look like if the data really came from a normal distribution? We can answer this by simulating data from a normal distribution using rnorm().\n\nset.seed(1)\nsim_norm &lt;- rnorm(n = length(fdims$hgt), mean = fhgtmean, sd = fhgtsd)\n\nHere the first argument indicates how many numbers you’d like to generate. We want this to be the same as the number of heights in the fdims dataset, which we can determine using the length function. The next two arguments to rnorm determine the mean and standard deviation of the normal distribution from which the simulated sample will be generated. We can take a look at the shape of our simulated dataset, sim_norm, as well as its normal probability plot.\n\n\n\n\n\n\nYour turn\n\n\n\nMake a normal Q-Q plot of sim_norm. Do all of the points fall on the line? How does this plot compare to the probability plot for the real data?\n\n\nEven better than comparing the original plot to a plot generated from a single sample from the normal distribution is to compare it to many more plots using the qqnormsim() function.\n\nqqnormsim(fdims$hgt)\n\n\n\n\n\n\n\nTip\n\n\n\nIt may be helpful to click the Zoom button in the plot window, in order to see these plots more clearly.\n\n\nThis command produces a 3x3 array of Q-Q plots: the first one (top-left) is the Q-Q plot of the data, which we have already seen above. The other eight plots arise from simulating random normal data with the same mean, standard deviation, and length as the data. (So if you run this command multiple times the first plot shouldn’t change, but the others will as new random numbers are used in the simulations each time.)\n\n\n\n\n\n\nYour turn\n\n\n\nDoes the normal probability plot for fdims$hgt look similar to the plots created for the simulated data? That is, do plots provide evidence that the female heights are nearly normal?\nNow analyse the data for female weights.Answer quiz questions 6 and 7.\n\n\nNormal probabilities\nOnce we decide that the distribution of values of a variable is approximately normal, we can answer all sorts of questions about that variable related to probability by modelling that variable as a normally distributed random variable. Take, for example, the question of, “What is the probability that a randomly chosen young adult female is taller than 164 cm?” 2\nIf we assume that female heights are normally distributed (a very close approximation is also okay), we can model the height in cm as a normally distributed random variable \\(H\\) with the mean given by the sample mean and the variance given by the sample variance. Then the probability that a randomly chosen young adult female is taller than 164 cm is given by \\[\n\\mathbb{P}\\left(H&gt;164\\right) = 1-\\mathbb{P}\\left(H\\leq 164\\right) = 1 - F_H(164)\\,.\n\\]\nIn R, the distribution function of a normal random variable is calculated with the function pnorm() and thus we obtain the desired probability with\n\n1 - pnorm(q = 164, mean = fhgtmean, sd = fhgtsd)\n#&gt; [1] 0.5530166\n\nThus assuming a normal distribution has allowed us to calculate a theoretical probability. If we instead want to calculate the probability empirically (using the data), we simply need to determine how many observations fall above 164, then divide this number by the total sample size, as you’ve seen before:\n\nsum(fdims$hgt &gt; 164) / length(fdims$hgt)\n#&gt; [1] 0.5153846\n\nAlthough the probabilities are not exactly the same, they are reasonably close. The closer that your distribution is to being normal, the more accurate the theoretical probabilities will be.\n\n\n\n\n\n\nYour turn\n\n\n\nAnswer quiz question 8.",
    "crumbs": [
      "Computer labs",
      "Lab 3: Data and distributions"
    ]
  },
  {
    "objectID": "labs/lab3.html#footnotes",
    "href": "labs/lab3.html#footnotes",
    "title": "Lab 3: Data and distributions",
    "section": "",
    "text": "Dekking, F.M.. A Modern Introduction to Probability and Statistics: Understanding Why and How. Springer, 2005.↩︎\nThe study that published this dataset is clear to point out that the sample was not random and therefore inference to a general population is not suggested. We do so here only as an exercise.↩︎",
    "crumbs": [
      "Computer labs",
      "Lab 3: Data and distributions"
    ]
  },
  {
    "objectID": "labs/lab4.html",
    "href": "labs/lab4.html",
    "title": "Lab 4: Sampling distributions",
    "section": "",
    "text": "Independent and identically distributed (i.i.d.) samples\nThis practical is about i.i.d. samples from random variables and drives home the fact that the sample mean and the sample variance are themselves random variables with a distribution. You will investigate this distribution and how it is affected by the sample size.\nAn i.i.d. sample of size \\(n\\) is a collection of \\(n\\) independent random variables \\(X_1,X_2,\\dots,X_n\\), each with the same distribution. (We use the notation \\(X\\) to refer to a generic random variable having the same distribution as any of the \\(X_i\\).) If we perform the probability experiment and measure all these random variables, we will get a collection of \\(n\\) numbers \\(x_1,x_2,\\dots,x_n\\). The topic of this worksheet is how to learn something about the distribution of the random variable \\(X\\) from the distribution of the values of this sample. This follows on from our investigations into simulation during Lab 1.\nWe saw in that lab that we can ask R to simulate an i.i.d. sample using the function sample(). There we did it for pulling coloured balls out of bags, or sampling faulty light bulbs. But it should come as no surprise that R can simulate random numbers from a wide range of discrete and continuous distributions. Let us here consider i.i.d. samples from the exponential distribution. Recall that if \\(X\\sim\\mbox{\\textup{Exp}}(\\lambda)\\) then its density function is \\[ f_X(x) = \\lambda e^{-\\lambda x}\\,, \\quad x\\ge 0 \\,. \\]\nThe following command simulates a measurement of a sample of size \\(n=100\\) from the exponential distribution with parameter \\(\\lambda =1\\).\nx &lt;- rexp(100, rate = 1)\nLet us look at the distribution of the values in this sample by making a histogram.\nhist(x)\nYou should now see a histogram displayed in the Plots panel at the lower right of the RStudio window. R automatically chooses the width of the bins in the histogram, based on the range of the data, using something called Sturge’s rule by default. We can suggest to R to use more bins if we want to get a more detailed view. We can also specify the probability = TRUE option so that the vertical axis shows the proportion of values in each bin instead of the number.\nhist(x, breaks = 20, probability = TRUE)\nLet us compare this to the density of the exponential distribution by plotting that on top of the histogram.\n# create a sequence of evenly  spaced values between 0 and 5\nxx &lt;- seq(0, 5, 0.05) \n# plot the density of the Exp(1) distribution at each value\nlines(xx, dexp(xx, rate = 1), col = \"blue\")\nNow let us assume we did not already know that this sample was from an exponential distribution with parameter 1. Let us instead assume that we were just given these 100 numbers, and believe that they come from an \\(\\mbox{\\textup{Exp}}(\\lambda)\\) distribution: we want to use these numbers to estimate \\(\\lambda\\).",
    "crumbs": [
      "Computer labs",
      "Lab 4: Sampling distributions"
    ]
  },
  {
    "objectID": "labs/lab4.html#estimating-the-expectation",
    "href": "labs/lab4.html#estimating-the-expectation",
    "title": "Lab 4: Sampling distributions",
    "section": "Estimating the expectation",
    "text": "Estimating the expectation\nIn this lab we will in particular look at estimating the expectation of the underlying distribution. We of course know that an exponentially distributed random variable \\(X\\) with parameter \\(\\lambda\\) has an expectation \\(\\mathbb{E}\\left[X\\right]=1/\\lambda\\). So if we can estimate \\(\\mathbb{E}\\left[X\\right]\\) we can also estimate the parameter \\(\\lambda\\).\nWe have seen that the sample mean \\[\\bar{X}_{n}=(X_1+\\cdots +X_{n})/n\\] is going to be a good estimator of the expectation of \\(X\\). The law of large numbers says that this estimator gets better and better as \\(n\\to\\infty\\). But for finite sample size \\(n\\) there will be some uncertainty in the estimate of \\(\\mathbb{E}\\left[X\\right]\\).\nLet us calculate the value \\(\\bar{x}_{100}\\) that the sample mean \\(\\bar{X}_{100}\\) takes in the realisation of the sample that we simulated above. When I ran mean(x) on my computer I obtained \\(\\bar{x}_{100}=\\) 0.9335716. However, you will get a different estimate for \\(\\mathbb{E}\\left[X\\right]\\) because the random number generator will have given you a different set of numbers \\(x_1,\\dots, x_{100}\\). As you know, you will get different random numbers each time you ask R for random numbers, except if you set the seed of the random number generator to a specific value right before you ask R for the random numbers. So if we run the code\n\nset.seed(1) \nx &lt;- rexp(100, rate = 1) \nmean(x)\n\nwe will all get the same set of values \\(x_1,\\dots,x_{100}\\), and thus the same estimate \\(\\mathbb{E}\\left[X\\right]\\approx\\) 1.0306764.\n\n\n\n\n\n\nYour turn\n\n\n\nAnswer quiz question 1.\n\n\nSampling distribution of the sample mean\nIt is not surprising that every time we take another realisation of the i.i.d. sample, we get a different value for the sample mean. After all, the sample mean \\(\\bar{X}_n\\) is a random variable. It is useful to get a sense of just how much variability we should expect when estimating the expectation value this way. The distribution of sample means \\(\\bar{X}_n\\), called the sampling distribution of the sample mean, can help us understand this variability.\nWe will visualise the sampling distribution of the sample mean by plotting the histogram from a large number of realisations of \\(\\bar{X}_n\\). Such a histogram gives an approximation to the probability density function of the random variable \\(\\bar{X}_n\\).\n\nsample_means50 &lt;- rep(0, 50000) \nset.seed(0) \nfor(i in 1:50000){\n  x &lt;- rexp(50, rate = 1) \n  sample_means50[i] &lt;- mean(x) \n}\n\nHere we use R to create 50,000 realisations of the samples of size \\(n=50\\), calculate the value of the sample mean \\(\\bar{x}_{50}\\) for each realisation, and store all the results in a vector called sample_means50. In the next subsection we’ll review how these lines of code work. For now let’s plot a histogram of the values:\n\nhist(sample_means50, breaks = 40, probability = TRUE)\n\nInterlude: the for loop\nLet’s take a break from the statistics for a moment to let that earlier block of code sink in. You have just run your first for loop, a cornerstone of computer programming. The idea behind the for loop is iteration: it allows you to execute code as many times as you want without having to type out every iteration. In the case above, we wanted to iterate the two lines of code inside the curly braces that simulates an i.i.d. sample of size 50 then save the mean of that realisation into the sample_means50 vector. Without the for loop, this would be painful:\n\nsample_means50 &lt;- rep(0, 50000) \nset.seed(0)\nx &lt;- rexp(50, rate = 1) \nsample_means50[1] &lt;- mean(x)\nx &lt;- rexp(50, rate = 1) \nsample_means50[2] &lt;- mean(x)\nx &lt;- rexp(50, rate = 1) \nsample_means50[3] &lt;- mean(x)\n\nand so on, 50,000 times.\nWith the for loop, these thousands of lines of code are compressed into a handful of lines.\nTo follow a bit more closely what is going on, I have added one extra line to the code below, which prints the variable i during each iteration of the loop. Run this code.\n\nsample_means50 &lt;- rep(0, 50000) \nset.seed(0) \nfor(i in 1:50000){ \n  x &lt;- rexp(50, rate = 1) \n  sample_means50[i] &lt;- mean(x) \n  print(i) \n}\n\nLet’s consider this code line by line to figure out what it does. In the first line we initialized a vector. In this case, we created a vector of 50,000 zeros called sample_means50. This vector will store values generated within the for loop. In the second line we set the seed for the random number generator.\nThe third line calls the for loop itself. The syntax can be loosely read as, “for every element i from 1 to 50,000, run the following lines of code”. You can think of i as the counter that keeps track of which step of the iteration you are on. Therefore, more precisely, the loop will run once when i=1, then once when i=2, and so on up to i=50000.\nThe body of the for loop is the part inside the curly braces, and this set of code is run for each value of i. Here, on every loop, we take an i.i.d. sample x of size 50 from the exponential distribution, take its mean, and store it as the ith element of sample_means50.\nIn order to display that this is really happening, we asked R to print i at each iteration. This line of code is optional and is only used for displaying what’s going on while the for loop is running.\nThe for loop allows us not only to run the code 50,000 times, but to neatly package the results, element by element, into the vector that we initialized at the outset.\n\n\n\n\n\n\nYour turn\n\n\n\nTo make sure you understand what you’ve done in this loop, try running a smaller version. Initialize a vector of 100 zeros called sample_means_small. Set the seed to 0 and run a loop that on each iteration takes a realisation of a sample of size 50 from the exponential distribution with parameter 1 and stores the sample mean in sample_means_small, but only iterate from 1 to 100. Print the output to your screen (type sample_means_small into the console and press Enter). How many elements are there in this object called sample_means_small? What does each element represent?Answer quiz question 2.\n\n\nSample size and the sampling distribution\nMechanics aside, let’s return to the purpose for running the loop: to compute a sampling distribution, specifically, this one:\n\nhist(sample_means50, breaks = 25, probability = TRUE) \n\nThe sampling distribution that we computed tells us much about estimating \\(\\mathbb{E}\\left[X\\right]\\). By looking at the histogram we see that most of the time we are going to get an estimate close to the true value of 1 but that there is some non-negligible probability of getting an estimate that is off by as much as 0.2.\nIndeed we can estimate the probability that the estimate is off by as much as 0.2 from the sampling distribution by calculating the proportion of the 50,000 values in sample_means50 that are more than 0.2 away from 1.\n\nmean(abs(sample_means50 - 1) &gt; 0.2) \n\nWe can also determine the empirical distribution function \\(F_{50}\\) for \\(\\bar{X}_{50}\\). This is a step function that jumps by \\(1/n\\) at every value that is represented in the sample (or by multiples of \\(1/n\\) if a value occurs multiple times), where \\(n\\) is the sample size. The function ecdf() returns the empirical distribution function:\n\nF50 &lt;- ecdf(sample_means50)\n\nLet’s plot this distribution function\n\nplot(F50)\n\n\n\n\n\n\n\nNote\n\n\n\nThe name ecdf stands for “empirical cumulative distribution function”. That it is called cumulative contains no extra information, it just so happens that some people refer to the distribution function as the “cumulative” distribution function to highlight the fact that a distribution function at \\(x\\) accumulates all the contributions from values up to \\(x\\).\n\n\nWe can now for example look at the probability that, when using a sample of size 50, we are going to get an estimate for \\(\\mathbb{E}\\left[X\\right]\\) that is more than 0.2 away from the true value of \\(\\mathbb{E}\\left[X\\right]=1\\).\n\\[\n  \\begin{split}\n    \\mathbb{P}\\left(|\\bar{X}_{50}-\\mathbb{E}\\left[X\\right]|&gt;0.2\\right) &= \\mathbb{P}\\left(\\bar{X}_{50}&lt;0.8\\right)+\\mathbb{P}\\left(\\bar{X}_{50}&gt;1.2\\right) \\\\\n    &= \\mathbb{P}\\left(\\bar{X}_{50}&lt;0.8\\right)+1-\\mathbb{P}\\left(\\bar{X}_{50}\\leq 1.2\\right) \\\\\n    &=F_{\\bar{X}_{50}}(0.8)+1-F_{\\bar{X}_{50}}(1.2).\n  \\end{split}\n\\]\nWe can estimate that from the empirical distribution function as follows:\n\nF50(0.8) + 1 - F50(1.2)\n\nThus, if we want to estimate \\(\\mathbb{E}\\left[X\\right]\\) with a precision of better than \\(\\pm 0.2\\) we should use a larger sample, because the chance of getting an answer outside the desired precision is about 16%.\n\n\n\n\n\n\nYour turn\n\n\n\nAnswer quiz question 3.\n\n\nTo get a sense of the effect that the sample size has on the sampling distribution, let’s build up two more sampling distributions: one based on a sample size of 100 and another based on a sample size of 200.\n\nsample_means100 &lt;- rep(0, 50000) \nsample_means200 &lt;- rep(0, 50000) \nset.seed(0) \nfor(i in 1:50000){ \n  x &lt;- rexp(100, rate=1) \n  sample_means100[i] &lt;- mean(x) \n  x &lt;- rexp(200, rate=1) \n  sample_means200[i] &lt;- mean(x) \n} \n\nHere we are able to use a single for loop to build two distributions by adding additional lines inside the curly braces. Don’t worry about the fact that x is used for the name of two different objects: in the second command of the for loop, the mean of x is saved to the relevant place in the vector sample_means100; with the mean saved, we’re now free to overwrite the object x with a new sample, this time of size 200. In general, any time you create an object using a name that is already in use, the old object will get replaced with the new one.\nTo see the effect that different sample sizes have on the sampling distribution, plot the three distributions above each other. First increase the size of the Plots pane in RStudio by clicking the icon at the top-right of the pane.\n\n\n\n\nThen use the commands\n\npar(mfrow = c(3, 1))\nxlimits = range(sample_means50)\nhist(sample_means50, breaks = 50, probability = TRUE, xlim = xlimits) \nhist(sample_means100, breaks = 50, probability = TRUE, xlim = xlimits) \nhist(sample_means200, breaks = 50, probability = TRUE, xlim = xlimits)\n\nThe first command specifies that you’d like to divide the plotting area into 3 rows and 1 column of plots. The breaks argument specifies the number of bins used in constructing the histogram. The xlim argument specifies the range of the x-axis of the histogram, and by setting it equal to xlimits for each histogram, we ensure that all three histograms will be plotted with the same limits on the x-axis.\nTo return to the default setting of plotting one plot at a time, run the following command:\n\npar(mfrow = c(1, 1))\n\n\n\n\n\n\n\nYour turn\n\n\n\nWhen the sample size is larger, what happens to the centre of the sampling distribution? What about the spread?Answer quiz question 4.\n\n\nLooking at the histogram for the sample mean of size 200, one gets the impression that it looks very much like a normal distribution. Why would the sample mean for the exponential distribution have a normal distribution? This is the Central Limit Theorem in action!",
    "crumbs": [
      "Computer labs",
      "Lab 4: Sampling distributions"
    ]
  },
  {
    "objectID": "labs/lab4.html#estimating-the-variance",
    "href": "labs/lab4.html#estimating-the-variance",
    "title": "Lab 4: Sampling distributions",
    "section": "Estimating the variance",
    "text": "Estimating the variance\nAs we will see in lectures, we can also estimate the variance \\(\\textup{Var}\\left(X\\right)\\) of a random variable from the sample variance. The sample variance is calculated with the R function var():\n\nx &lt;- rexp(50, rate = 1)\nvar(x)\n\n\n\n\n\n\n\nYour turn\n\n\n\nEvaluate the above code a few times to see the wide range of estimates we get. Then set the seed to 0 and use a for loop to create 50,000 realisations of the i.i.d. sample of size 50 from an \\(\\mbox{\\textup{Exp}}(1)\\) distribution, calculate the variance of each and store these in an array sample_variances50. Then plot a histogram of these values of the sample variance using the command hist(sample_variances50, breaks=50, probability=TRUE).Answer quiz question 5.\n\n\nNotice that the sampling distribution for the sample variance does not look at all like a normal distribution. The central limit theorem can be used to show that for sufficiently large sample sizes the sampling distribution will again look normal; however a sample size of 50 is clearly not enough for that.",
    "crumbs": [
      "Computer labs",
      "Lab 4: Sampling distributions"
    ]
  },
  {
    "objectID": "labs/lab4.html#real-estate-data",
    "href": "labs/lab4.html#real-estate-data",
    "title": "Lab 4: Sampling distributions",
    "section": "Real-estate data",
    "text": "Real-estate data\nNow let’s look at some real data. This week we’ll consider real-estate data from the city of Ames, Iowa. The details of every real estate transaction in Ames is recorded by the City Assessor’s office.\nOur particular focus for this lab will be all residential home sales in Ames between 2006 and 2010. Let’s load the data.\n\ndownload.file(\"http://www.openintro.org/stat/data/ames.RData\", \n              destfile = \"ames.RData\")\nload(\"ames.RData\")\n\nAs you can see in the Environment panel in RStudio, there is now a variable ames with 2930 observations of 82 variables. Let’s take a look at the names of the variables in this dataset.\n\nnames(ames)\n\nWe will focus our attention on two of the variables: the above ground living area of the house in square feet (Gr.Liv.Area) and the sale price (SalePrice). To save some effort throughout the lab, create two variables with short names that represent these two variables.\n\narea &lt;- ames$Gr.Liv.Area\nprice &lt;- ames$SalePrice\n\nWe refer to the collection of all the house sales in Ames as the “population”. The term “population” is used by statisticians not only to refer to populations of people but to any complete collection of observations.\n\n\n\n\n\n\nYour turn\n\n\n\nAnswer quiz question 6.\n\n\nIn this lab we have access to the entire population, but this is rarely the case in real life. Gathering information on an entire population is often extremely costly or impossible. Because of this, we often take a sample of the population and use that to understand the properties of the population.\nTaking a sample\nIf we were interested in estimating the mean living area of houses sold in Ames, but did not have access to the data from all house sales, we could randomly select a smaller number of sales to survey and collect the data only for those. Then we could use that sample as the basis of our estimation. Let us assume we only have enough resources to observe 50 randomly selected house sales. We can simulate taking such a random sample with the command\n\nsamp1 &lt;- sample(area, 50)\n\nThis command randomly chooses 50 entries from the vector area, and this is then assigned to the variable samp1. (You will remember the sample() function from Lab 1, where we used it to generate random numbers by making it sample from the entries of the vector 1:6. The difference here is that we are using the sample() function without the replace=TRUE option, so that we do not get the same element more than once.)\nTheoretically we model such a sample with a sequence of i.i.d. random variables \\(X_1,\\dots, X_{50}\\). This is now a sample from the distribution of the area among all houses.\n\n\n\n\n\n\nNote\n\n\n\nBecause we are sampling without replacement from a finite population, the \\(X_i\\) are not strictly independent. But we often assume that the population is so large compared to the sample size that the dependence is negligible.\n\n\nThe vector samp1 contains a particular realisation of those random variables \\(X_1,\\dots, X_{50}\\). The estimator that we use to estimate the average living area in homes in Ames is the sample mean, i.e., the random variable \\[\\bar{X}_{50}=(X_1+\\cdots + X_{50})/50.\\] For our realisation of the sample it takes the value\n\nmean(samp1)\n\nDepending on which 50 homes you randomly selected, your estimate could be a bit above or a bit below the true population mean of 1499.69 square feet. In general, though, the sample mean turns out to be a pretty good estimate of the average living area, and we were able to get it by sampling less than 3% of the population.\n\n\n\n\n\n\nYour turn\n\n\n\nTake a second realisation of the sample, also of size 50, and call it samp2. How does the mean of samp2 compare with the mean of samp1? Take two more samples, one of size 100 and one of size 1,000. Which would you think would provide a more accurate estimate of the “true” mean?Answer quiz question 7.\n\n\nSampling distribution\nNot surprisingly, every time we take another random sample, we get a different value for the sample mean. It is useful to get a sense of just how much variability we should expect when estimating the expected value this way, just as when we were working with simulated data from an exponential distribution earlier in this lab. So we again want to understand the sampling distribution of the sample mean \\(\\bar{X}_{50}\\).\nWe will visualise the sampling distribution by plotting the histogram for 5,000 realisations of \\(\\bar{X}_{50}\\). As we know, such a histogram gives an approximation to the probability density function.\n\n\n\n\n\n\nWarning\n\n\n\nPossibility of confusion: we are taking a sample of size 5,000 from the sampling distribution of the sample mean of a sample of size 50!\n\n\n\nset.seed(12)\narea_sample_means50 &lt;- rep(0, 5000)\nfor(i in 1:5000){ \n  samp &lt;- sample(area, 50) \n  area_sample_means50[i] &lt;- mean(samp) \n}\nhist(area_sample_means50, breaks = 25, probability = TRUE)\n\nHere we use R to create 5,000 realisations of the samples of size 50, calculate the value of the sample mean for each, and store each result in a vector called area_sample_means50.\n\n\n\n\n\n\nYour turn\n\n\n\nAnswer quiz question 8. (Be careful not to overwrite the variable area_sample_means50 because we still want to use it below.)\n\n\nThe sampling distribution that we computed tells us much about estimating the average living area of homes in Ames. Because the sample mean is an unbiased estimator (we’ll be learning about what this means next week!), the sampling distribution has its mean at the true average living area, and the spread of the distribution indicates how much variability is induced by sampling only 50 homes.\nWe can also determine the empirical distribution function \\(F_{50}\\) for \\(\\bar{X}_{50}\\):\n\nFarea50 &lt;- ecdf(area_sample_means50) \nplot(Farea50) \n\nWe can now for example look at the probability that, when using a sample of size 50, we are going to get an estimate for \\(\\mathbb{E}\\left[X\\right]\\) that is more than 100 away from the true population mean area of 1499.69:\n\\[\n  \\begin{split}\n    \\mathbb{P}\\left(|\\bar{X}_{50}-1499.69|&gt;100\\right) &= \\mathbb{P}\\left(\\bar{X}_{50}&lt;1399.69\\right)+ \\mathbb{P}\\left(\\bar{X}_{50}&gt;1599.69\\right) \\\\\n    &= \\mathbb{P}\\left(\\bar{X}_{50}&lt;1399.69\\right)+1-\\mathbb{P}\\left(\\bar{X}_{50}\\leq 1599.69\\right) \\\\\n    &= F_{\\bar{X}_{50}}(1399.69)+1-F_{\\bar{X}_{50}}(1599.69).\n  \\end{split}\n\\]\nWe can estimate that from the empirical distribution function as follows:\n\nFarea50(1399.69) + 1 - Farea50(1599.69) \n#&gt; [1] 0.1612\n\n(If you used a different random seed before producing area_sample_means50 then you’ll get a slightly different answer here, of course.) Thus, if we want to estimate \\(\\mathbb{E}\\left[X\\right]\\) with a precision of better than \\(\\pm\\) 100 we should use a larger sample, because the chance of getting an answer outside the desired accuracy is about 16%.\n\n\n\n\n\n\nYour turn\n\n\n\nAnswer quiz question 9.\n\n\nEffect of the size of the sample\nTo get a sense of the effect that sample size has on the sampling distribution, let’s build up two more sampling distributions: one based on a sample size of 10 and another based on a sample size of 100.\n\narea_sample_means10 &lt;- rep(0, 5000) \narea_sample_means100 &lt;- rep(0, 5000)\nfor(i in 1:5000){ \n  samp &lt;- sample(area, 10) \n  area_sample_means10[i] &lt;- mean(samp) \n  samp &lt;- sample(area, 100) \n  area_sample_means100[i] &lt;- mean(samp) \n}\n\n\n\nTo see the effect that different sample sizes have on the sampling distribution, we plot the three distributions above each other.\n\npar(mfrow = c(3, 1))\nxlimits &lt;- range(area_sample_means10)\nhist(area_sample_means10, breaks = 20, xlim = xlimits) \nhist(area_sample_means50, breaks = 20, xlim = xlimits) \nhist(area_sample_means100, breaks = 20, xlim = xlimits) \npar(mfrow = c(1, 1))\n\nQuestions:\n\nWhen the sample size is larger, what happens to the centre?\nWhat about the spread?\n\nAnswers:\n\nThe centre does not really change. We are using an unbiased estimator, so the expectation of the sampling distribution always stays at the true population mean.\nThe spread decreases. More precisely, the variance of the sampling distribution is inversely proportional to the sample size. So as the sample size changes by a factor of 10, the variance of the sampling distribution should change by a factor of 1/10.\n\nLet us check this last claim:\n\nvar(area_sample_means10)/var(area_sample_means100)\n\nWhy is the ratio not exactly 10? This is because we are only estimating the variance of the sampling distribution from the sample variance in a sample of size 5,000 and that estimate is not exact.",
    "crumbs": [
      "Computer labs",
      "Lab 4: Sampling distributions"
    ]
  },
  {
    "objectID": "labs/lab4.html#you-only-have-one-sample",
    "href": "labs/lab4.html#you-only-have-one-sample",
    "title": "Lab 4: Sampling distributions",
    "section": "You only have one sample",
    "text": "You only have one sample\nIn practice, of course, you will only have the resources to take a single sample. You will not be able to build up a picture of the sampling distribution as we have done here by taking thousands of samples. But without knowing the sampling distribution, you have no way of knowing how precise your estimate is likely to be. This is a problem.\nHow confident can you be in your estimate from a sample if you do not know the sampling distribution? This is something we will discuss in lectures.",
    "crumbs": [
      "Computer labs",
      "Lab 4: Sampling distributions"
    ]
  },
  {
    "objectID": "assignments/assignments.html",
    "href": "assignments/assignments.html",
    "title": "Written assignments",
    "section": "",
    "text": "Assignment sheets will appear on the VLE as pdf files. If you would prefer to view an html version then you can find links to these below. (Each link will only work once the relevant sheet has been released on the VLE.)\n\n\n\nAssignment\nAQ Solutions\nFull Solutions\n\n\n\n\nAssignment 1\nAssignment 1\nAssignment 1\n\n\nAssignment 2\nAssignment 2\nAssignment 2\n\n\nAssignment 3\nAssignment 3\nAssignment 3\n\n\nAssignment 4\nAssignment 4\nAssignment 4\n\n\nAssignment 5\nAssignment 5\nAssignment 5",
    "crumbs": [
      "Written assignments"
    ]
  }
]